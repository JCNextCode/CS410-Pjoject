24
Chapter 2
Background
{
}
,
,
,
,
,
where the colors of the shapes are, red, orange, yellow, blue, green, and purple,
respectively.
We can now create another distribution for the shape θS. Let each index in θS
represent p(square), p(circle), p(triangle), respectively. That gives
θS =
�1
3, 1
2 , 1
6
�
.
Then we can let xC ∼ θC represent the color random variable and let xS ∼ θS repre-
sent the shape random variable. We now have two variables to work with.
A joint probability measures the likelihood that two events occur simultane-
ously. For example, what is the probability that xC = red and xS = circle? Since there
are no red circles, this has probability zero. How about p(xC = green, xS = circle)?
This notation signifies the joint probability of the two random variables. In this
case, the joint probability is 1
6 because there is only one green circle.
Consider a modified die:
{
}
,
,
,
,
,
where we changed the color of the blue circle (the fourth element in the set) to
green. Thus, we now have two green circles instead of one green and one blue.
What would p(xC = green, xS = circle) be? Since two out of the six elements satisfy
both these criteria, the answer is 2
6 = 1
3. As another example, if we had a 12-sided
fair die with 5 green circles and 7 other combinations of shape and color, then
p(xC = green, xS = circle) = 5
12.
A conditional probability measures the likelihood that one event occurs given
that another event has already occurred. Let’s use the original die with six unique
colors. Say we know that a square was rolled. With that information, what is the
probability that the color is red? How about purple? We can write this as p(xC =
red | xS = square). Since we know there are two squares, of which one is red, p(red |
square) = 1
2.
We can write the conditional probabilities for two random variables X and Y
based on their joint probability with the following equation:
p(X = x | Y = y) = p(X = x, Y = y)
p(Y = y)
.
(2.4)
