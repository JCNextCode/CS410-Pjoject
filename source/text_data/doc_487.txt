B.3 Incomplete vs. Complete Data
467
The right side of this equation is easily seen to be a function with p(w | θF) as
variables. To find ˆθF, we can, in principle, use any optimization methods. Since
the function involves a logarithm of a sum of two terms, it is difficult to obtain a
simple analytical solution via the Lagrange Multiplier approach, so in general, we
must rely on numerical algorithms. There are many possibilities; EM happens to
be just one of them which is quite natural and guaranteed to converge to a local
maxima, which, in our case, is also a global maximum, since the likelihood function
can be shown to have one unique maximum.
B.3
Incomplete vs. Complete Data
The main idea of the EM algorithm is to “augment” our data with some latent
variables so that the “complete” data has a much simpler likelihood function—
simpler for the purpose of finding a maximum. The original data are thus treated as
“incomplete.” As we will see, we will maximize the incomplete data likelihood (our
original goal) through maximizing the expected complete data likelihood (since
it is much easier to maximize) where expectation is taken over all possible values
of the hidden variables (since the complete data likelihood, unlike our original
incomplete data likelihood, would contain hidden variables).
In our example, we introduce a binary hidden variable z for each occurrence of a
word w to indicate whether the word has been “generated” from the background
model p(w | C) or the topic model p(w | θF). Let dij be the jth word in document
di. We have a corresponding variable zij defined as follows:
zij =
� 1
if word dij is from background
0
otherwise.
We thus assume that our complete data would have contained not only all the
words in F, but also their corresponding values of z. The log-likelihood of the
complete data is thus
Lc(θF) = log p(F , z | θF)
=
k
�
i=1
|di|
�
j=1
[(1 − zij) log((1 − λ)p(dij | θF)) + zij log(λp(dij | C))].
Note the difference between Lc(θF) and L(θF): the sum is outside of the log-
arithm in Lc(θF), and this is possible because we assume that we know which
component model has been used to generated each word dij.
What is the relationship between Lc(θF) and L(θF)? In general, if our parameter
is θ, our original data is X, and we augment it with a hidden variable H, then
