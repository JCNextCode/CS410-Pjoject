11.3 Evaluation of Recommender Systems
233
Filtering can also be considered a much harder task because you have to make
a binary decision and can’t afford waiting for many items to enhance your belief
that one is better than others. Let’s think about news filtering as soon as the system
detects the news articles: you have to decide whether the news would be interesting
to a user. If you wait for a few days, even an accurate recommendation of the
most relevant news is not interesting. Another reason why it’s hard is due to data
sparseness. If you think of this as a learning problem in collaborative filtering, for
example, it’s purely based on learning from the past ratings. If you don’t have many
ratings, there’s really not much you can do. As we mentioned, there are strategies
that have been proposed to solve the problem. For example, we can use more user
information to assess their similarity instead of just using the item preferences.
We also talked about the two strategies for a filtering task; one is content-based
where we look at item content similarity. The other is user similarity, which is
collaborative filtering. We talked about push vs. pull as two strategies for getting
access to the text data. Recommender systems aid users in push mode whereas
search engines assist users in pull mode.
11.3
Evaluation of Recommender Systems
In evaluation setups for collaborative filtering, we have a set P of pairs of predicted
ratings ˆr and actual ratings r across all user-item pairs. A very common measure
called root-mean squared error (RMSE) has a mathematical formula that follows
its name:
RMSE(P) =
�
1
|P|
�
(ˆr,r)∈P
(ˆr − r)2.
(11.6)
A similar metric is mean absolute error (MAE), defined as
MAE(P) =
�
1
|P|
�
(ˆr,r)∈P
|ˆr − r|.
(11.7)
Due to the square in RMSE, RMSE is more affected by larger errors. The similarity
between RMSE and MAE should remind the reader of the differences between
gMAP and MAP. Both these measures quantify the difference in values between
ˆr and the true rating r. Using such a measure is natural when we have ratings on
some ordinal scale.
One important note is that these measures only capture the accuracy of predicted
ratings; in an actual recommender system, the top k elements are recommended
to the user. Despite having a low RMSE or MAE, it may be possible that the top
