260
Chapter 13
Word Association Mining
idea of a weighted vector can also be assumed to be candidates for syntagmatic
relations.
Of course, this is only a byproduct of our approach for discovering paradigmatic
relations. In the next section, we’ll talk more about how to discover syntagmatic
relations in particular. This discussion clearly shows the relation between discover-
ing the two relations. Indeed, these two word relations may be discovered in a joint
manner by leveraging such associations. This also shows some interesting connec-
tions between the discovery of syntagmatic relations and paradigmatic relations.
Specifically, words that are paradigmatically related tend to have a syntagmatic re-
lation with the same word.
To summarize, the main idea of computing paradigmatic relations is to collect
the context of a candidate word to form a pseudo document which is typically
represented as a bag of words. We then compute the similarity of the corresponding
context documents of two candidate words; highly similar word pairs have the
highest paradigmatic relations, i.e., the words that share similar contexts. There are
many different ways to implement this general idea, but we just talked about a few
of the approaches. Specifically, we talked about using text retrieval models to help
us design an effective similarity function to compute the paradigmatic relations.
More specifically, we used BM25 TF and IDF weighting to discover paradigmatic
relations. Finally, syntagmatic relations can also be discovered as a byproduct when
we discover paradigmatic relations.
13.3
Discovery of Syntagmatic Relations
There are strong syntagmatic relations between words that have correlated co-
occurrences. That means when we see one word occur in some context, we tend
to see the other word.
Consider a more specific example shown in Figure 13.7. We can ask the ques-
tion, whenever eats occurs, what other words also tend to occur? Looking at the
sentences on the left, we see some words that might occur together with eats, like
cat, dog, or fish. If we remove them and look at where we only show eats surrounded
by two blanks, can we predict what words occur to the left or to the right?
If these words are associated with eats, they tend to occur in the context of eats.
More specifically, our prediction problem is to take any text segment (which can be
a sentence, paragraph, or document) and determine what words are most likely to
co-occur in a specific context.
Let’s consider a particular word w. Is w present or absent in the segment from
Figure 13.8? Some words are actually easier to predict than other words—if you
