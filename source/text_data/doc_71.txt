3.4 Statistical Language Models
51
ing general conversations, but it may be inaccurate for describing conversations
happening at a mathematics conference, where the sequence The equation has a
solution may occur more frequently than Today is Wednesday.
Given a language model, we can sample word sequences according to the distri-
bution to obtain a text sample. In this sense, we may use such a model to “generate”
text. Thus, a language model is also often called a generative model for text.
Why is a language model useful? A general answer is that it provides a principled
way to quantify the uncertainties associated with the use of natural language.
More specifically, it allows us to answer many interesting questions related to text
analysis and information retrieval. The following are some examples of questions
that a language model can help answer.
.
Given that we see John and feels, how likely will we see happy as opposed to
habit as the next word? Answering this question can help speech recognition
as happy and habit have very similar acoustic signals, but a language model
can easily suggest that John feels happy is far more likely than John feels habit.
.
Given that we observe baseball three times and game once in a news article,
how likely is it about the topic “sports”? This will obviously directly help text
categorization and information retrieval tasks.
.
Given that a user is interested in sports news, how likely would it be for the
user to use baseball in a query? This is directly related to information retrieval.
If we enumerate all the possible sequences of words and give a probability to
each sequence, the model would be too complex to estimate because the number
of parameters is potentially infinite since we have a potentially infinite number
of word sequences. That is, we would never have enough data to estimate these
parameters. Thus, we have to make assumptions to simplify the model.
The simplest language model is the unigram language model in which we
assume that a word sequence results from generating each word independently.
Thus, the probability of a sequence of words would be equal to the product of the
probability of each word. Formally, let V be the set of words in the vocabulary, and
w1, . . . , wn a word sequence, where wi ∈ V is a word. We have
p(w1, . . . , wn) =
n
�
i=1
p(wi).
(3.1)
Given a unigram language model θ, we have as many parameters as the words
in the vocabulary, and they satisfy the constraint �
w∈V p(w) = 1. Such a model
essentially specifies a multinomial distribution over all the words.
