464
Appendix A
Bayesian Statistics
smoothing would be now? It’s
⃗μ =
�
μp(w1 | C), μp(w2 | C), . . . , μp(wk | C)
�
(A.17)
In other words, the Dirichlet prior for this smoothing method is proportional to
the background, collection language model.
Looking back at Add-1 smoothing, we can imagine this as a special case of
Dirichlet prior smoothing. If we drew the uniform distribution from our Dirichlet,
we’d get
p(w | d) = c(w, d) + 1
|d| + |V |
(A.18)
This implies that each word is equally likely in our collection language model,
which is most likely not the case. Note |V | = k, or �k
i=1 1 since ⃗μ = {1, 1, . . . , 1}.
A.6
Conclusion
Starting with the Bernoulli distribution for a single coin flip, we expanded it into a
set of trials with the binomial distribution. We investigated parameter estimation
via MLE, and then moved onto a Bayesian approach. We compared the Bayesian re-
sult to smoothing with pseudo counts and saw how hyperparameters affected the
distribution. Once we had this foundation, we moved onto multidimensional dis-
tributions capable of representing individual words. We saw how the Beta-binomial
model is related to the Dirichlet-multinomial model, and inspected it in the context
of Dirichlet prior smoothing for query likelihood in IR.
