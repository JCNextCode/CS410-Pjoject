98
Chapter 6
Retrieval Models
q = (x1, …, xN)
Sim(q, d) = q.d = x1y1 + … + xNyN = ΣN
i=1 xi yi
xi = count of word Wi in query
yi = count of word Wi in doc
d = (y1, …, yN)
Figure 6.7
Frequency vector representation and dot product similarity.
q =
d2 =
(1,
(1,
1,
1,
1,
0,
1,
1,
0,
1,
…)
…)
d2
… news about organic food campaign …
q =
d3 =
(1,
(1,
1,
0,
1,
1,
1,
1,
0,
0,
…)
…)
q =
d4 =
(1,
(1,
1,
0,
1,
2,
1,
1,
0,
0,
…)
…)
f(q, d2) = 3
f(q, d3) = 3
f(q, d4) = 4!
d3
… news of presidential campaign …
d4
… news of presidential campaign …
… presidential candidate …
Figure 6.8
Frequency vector representation rewards multiple occurrences of a query term.
Now, let’s see what the formula would look like if we change this representation.
The formula looks identical since we are still using the dot product similarity. The
difference is inside of the sum since xi and yi are now different—they’re now the
counts of words in the query and the document. Because of the change in document
representation, the new score has a different interpretation. We can see whether
this would fix the problems of the bit vector VS model.
Look at the three documents again in Figure 6.8. The query vector is the same
because all these words occurred exactly once in the query. The same goes for d2
and d3 since none of these words has been repeated. As a result, the score is also
the same for both these documents. But, d4 would be different; here, presidential
occurred twice. Thus, the corresponding dimension would be weighted as two
instead of one, and the score for d4 is higher. This means, by using TF, we can
now rank d4 above d2 and d3 as we had hoped to.
