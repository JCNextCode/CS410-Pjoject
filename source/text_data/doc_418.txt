398
Chapter 18
Opinion Mining and Sentiment Analysis
p(r ≥ j|X) = 
p(r ≥ k|X) > 0.5? 
eαj+∑M
i=1xiβji
—
eαj+∑M
i=1xiβji + 1
No
Yes
r = k
r = 2
r = 1
j = k, k – 1, …, 2
After training k – 1
Logistic regression classiﬁers
r = k – 1
p(r ≥ k – 1|X) > 0.5? 
No
…
Yes
p(r ≥ 2|X) > 0.5? 
No
Yes
Text object: X = (x1, x2, …, xM), xi 2 <
Rating: r 2 {1, 2, …, k}
Figure 18.6
Multi-level logistic regression for sentiment analysis: prediction of ratings.
With this modification, each classifier needs a different set of parameters, yield-
ing many more parameters overall. We will index the logistic regression classifiers
by an index j, which corresponds to a rating level. This is to make the notation more
consistent with what we show in the ordinal logistic regression. So, we now have
k − 1 regular logistic regression classifiers, each with its own set of parameters.
With this approach, we can now predict ratings, as shown in Figure 18.6.
After we have separately trained these k − 1logistic regression classifiers, we can
take a new instance and then invoke classifiers sequentially to make the decision.
First, we look at the classifier that corresponds to the rating level k. This classifier
will tell us whether this object should have a rating of k or not. If the probability
according to this logistic regression classifier is larger than 0.5, we’re going to say
yes, the rating is k. If it’s less than 0.5, we need to invoke the next classifier, which
tells us whether it’s at least k − 1. We continue to invoke the classifiers until we hit
the end when we need to decide whether it’s 2 or 1.
Unfortunately, such a strategy is not an optimal way of solving this problem.
Specifically, there are two issues with this approach. The first problem is that there
are simply too many parameters. For each classifier, we have M + 1parameters with
k − 1 classifiers all together, so the total number of parameters is (k − 1) . (M + 1).
When a classifier has many parameters, we would in general need more training
data to help us decide the optimal parameters of such a complex model.
The second problem is that these k − 1 classifiers are not really independent.
We know that, in general, words that are positive would make the rating higher for
any of these classifiers, so we should be able to take advantage of this fact. This is
