A
APPENDIX
Bayesian Statistics
Here, we examine Bayesian statistics in more depth as a continuation of Chapter 2
and Chapter 17.
A.1
Binomial Estimation and the Beta Distribution
From section 2.1.5, we already know the likelihood of our binomial distribution is
p(D | θ) = θH(1 − θ)T ,
(A.1)
but what about the prior, p(θ)? A prior should represent some “prior belief” about
the parameters of the model. For our coin flipping (i.e., binomial distribution), it
would make sense to have the prior also be proportional to the powers of θ and
(1 − θ). Thus, the posterior will also be proportional to those powers:
p(θ | D) ∝ p(θ)p(D | θ)
= θa(1 − θ)bθH(1 − θ)T
= θa+H(1 − θ)b+T .
So we need to find some distribution of the form P(θ) ∝ θa(1 − θ)b. Luckily, there
is something called the Beta distribution. We say x ∼ Beta(α, β) if for x ∈ [0, 1]
p(x | α, β) = �(α + β)
�(α)�(β)xα−1(1 − x)β−1.
(A.2)
This is the probability density function (pdf) of the Beta distribution. But what is
�(α + β)
�(α)�(β)?
(A.3)
The �(.) is the Gamma function. It can be thought of as the continuous version
of the factorial function. That is, �(x) = (x − 1)�(x − 1). Or rather for an x ∈ Z+,
�(x) = (x − 1)!. That still doesn’t explain the purpose of that constant in front of
xα−1(1 − x)β−1.
