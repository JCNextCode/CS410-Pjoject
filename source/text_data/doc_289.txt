13.3 Discovery of Syntagmatic Relations
269
p(Xw1 = 1) =
Segment_1
Segment_2
Segment_3
Segment_4
 …
Segment_N
count(w1) = total number segments that contain w1
count(w2) = total number segments that contain w2
count(w1, w2) = total number segments that contain both w1 and w2
w1
1
1
1
0
0
w2
0
1
1
0
1
Only w1 occurred
Both occurred
Both occurred
Neither occurred
Only w2 occurred
count(w1)
—
N
p(Xw2 = 1) = count(w2)
—
N
p(Xw1 = 1, Xw2 = 1) = count(w1, w2)
—
N
Figure 13.14
Estimation of probabilities involved in the definition of mutual information.
the maximum likelihood estimate (MLE), where we simply normalize the observed
counts. Using MLE, we can compute these probabilities as follows. For estimating
the probability that we see a word occuring in a segment, we simply normalize the
count of segments that contain this word. On the right side of Figure 13.14, you see
a list of some segments of data. In some segments you see both words occur, which
is indicated as ones for both columns. In some other cases only one will occur, so
only that column has a one and the other column has a zero.
To estimate these probabilities, we simply need to collect the three counts: the
count of w1 (the total number of segments that contain w1), the segment count for
w2, and the count when both words occur (both columns have ones). Once we have
these counts, we can just normalize these counts by N, which is the total number of
segments, giving us the probabilities that we need to compute mutual information.
There is a small problem when we have zero counts sometimes. In this case,
we don’t want a zero probability, so we use smoothing, as discussed previously in
this book.
To smooth, we will add a small constant to these counts so that we don’t get zero
probability in any case. Smoothing for this application is displayed in Figure 13.15.
We pretend to observe pseudo-segments that would contribute additional counts
of these words so that no event will have zero probability. In particular for this exam-
ple, we introduce four pseudo-segments. Each is weighted at 1/4. These represent
the four different combinations of occurrences of the two words.
Each combination will have at least a non-zero count from a pseudo-segment;
thus, in the actual segments that we’ll observe, it’s okay if we haven’t observed all of
