13.3 Discovery of Syntagmatic Relations
267
I(Xw1; Xw2) = 
Presence and absence of w1:  p(Xw1 = 1) + p(Xw1 = 0) = 1
Presence and absence of w2:  p(Xw2 = 1) + p(Xw2 = 0) = 1
Both w1 and w2 occur
Co-occurrences of w1 and w2:
Only w1 occurs
Only w2 occurs
None of them occurs
∑
u2{0,1} ∑
v2{0,1}
p(Xw1 = u, Xw2 = v)log2
p(Xw1 = 1, Xw2 = 1) + p(Xw1 = 1, Xw2 = 0) + p(Xw1 = 0, Xw2 = 1) + p(Xw1 = 0, Xw2 = 0) = 1 
p(Xw1 = u, Xw2 = v)
—
p(Xw1 = u)p(Xw2 = v)
Figure 13.11
Probabilities involved in the definition of mutual information.
Presence and absence of w1:    p(Xw1 = 1) + p(Xw1 = 0) = 1
Presence and absence of w2:    p(Xw2 = 1) + p(Xw2 = 0) = 1
Co-occurrences of w1 and w2:
Constraints:
p(Xw1 = 1, Xw2 = 1) + p(Xw1 = 1, Xw2 = 0) + p(Xw1 = 0, Xw2 = 1) + p(Xw1 = 0, Xw2 = 0) = 1 
p(Xw1 = 1, Xw2 = 1) + p(Xw1 = 1, Xw2 = 0) = p(Xw1 = 1)
p(Xw1 = 0, Xw2 = 1) + p(Xw1 = 0, Xw2 = 0) = p(Xw1 = 0)
p(Xw1 = 1, Xw2 = 1) + p(Xw1 = 0, Xw2 = 1) = p(Xw2 = 1)
p(Xw1 = 1, Xw2 = 0) + p(Xw1 = 0, Xw2 = 0) = p(Xw2 = 0) 
Figure 13.12
Constraints on probabilities in the mutual information function.
or absence of this word. These all sum to one as well. Finally, we have a lot of joint
probabilities that represent the scenarios of co-occurrences of the two words. They
also sum to one because the two words can only have the four shown possible sce-
narios. Once we know how to calculate these probabilities, we can easily calculate
the mutual information.
It’s important to note that there are some constraints among these probabilities.
The first was that the marginal probabilities of these words sum to one. The second
was that the two words have these four scenarios of co-occurrence. The additional
constraints are listed at the bottom of Figure 13.12.
