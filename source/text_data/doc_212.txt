192
Chapter 10
Web Search
techniques that have been developed specifically for web search. One such tech-
nique is parallel indexing and searching. This addresses the issue of scalability; in
particular, Google’s MapReduce framework is very influential.
There are also techniques that have been developed for addressing the spam
problem. We’ll have to prevent those spam pages from being ranked high. There are
also techniques to achieve robust ranking in the light of search engine optimizers.
We’re going to use a wide variety of signals to rank pages so that it’s not easy to
spam the search engine with one particular trick.
The third line of techniques is link analysis; these are techniques that can allow
us to improve search results by leveraging extra information about the networked
nature of the web. Of course, we will use multiple features for ranking—not just
link analysis. We can also exploit all kinds of features like the layout of web pages
or anchor text that describes a link to another page.
The first component of a web search engine is the crawler. This is a program
that downloads web page content that we wish to search. The second component
is the indexer, which will take these downloaded pages and create an inverted
index. The third component is retrieval, which answers a user’s query by talking
to the user’s browser. The browser will show the search results and allow the user
to interact with the web. These interactions with the user allow opportunities for
feedback (discussed in Chapter 7) and evaluation (discussed in Chapter 9). In the
next section, we will discuss crawling. We’ve already described all indexing steps
except crawling in detail in Chapter 8.
After our crawling discussion, we move onto the particular challenges of web
indexing. Then, we discuss how we can take advantage of links between pages in
link analysis. The last technique we discuss is learning to rank, which is a way to
combine many different features for ranking.
10.1
Web Crawling
The crawler is also called a spider or a software robot that crawls (traverses, parses,
and downloads) pages on the web. Building a toy crawler is relatively easy because
you just need to start with a set of seed pages, fetch pages from the web, and parse
these pages’ new links. We then add them to a queue and then explore those page’s
links in a breadth-first search until we are satisfied.
Building a real crawler is quite tricky and there are some complicated issues
that we inevitably deal with. One issue is robustness: What if the server doesn’t
respond or returns unparseable garbage? What if there’s a trap that generates
dynamically generated pages that attract your crawler to keep crawling the same
