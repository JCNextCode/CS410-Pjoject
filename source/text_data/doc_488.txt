468
Appendix B
Expectation- Maximization
p(X, H | θ) = p(H | X, θ)p(X | θ). Thus,
Lc(θ) = log p(X, H | θ) = log p(X | θ) + log p(H | X, θ) = L(θ) + log p(H | X, θ).
B.4
A Lower Bound of Likelihood
Algorithmically, the basic idea of EM is to start with some initial guess of the pa-
rameter values θ(0) and then iteratively search for better values for the parameters.
Assuming that the current estimate of the parameters is θ(n), our goal is to find
another θ(n+1) that can improve the likelihood L(θ).
Let us consider the difference between the likelihood at a potentially better
parameter value θ and the likelihood at the current estimate θ(n), and relate it with
the corresponding difference in the complete likelihood:
L(θ) − L(θ(n)) = Lc(θ) − Lc(θ(n)) + log p(H | X, θ(n))
p(H | X, θ) .
(B.3)
Our goal is to maximize L(θ) − L(θ(n)), which is equivalent to maximizing L(θ).
Now take the expectation of this equation w.r.t. the conditional distribution of the
hidden variable given the data X and the current estimate of parameters θ(n), i.e.,
p(H | X, θ(n)). We have
L(θ) − L(θ(n)) =
�
H
Lc(θ)p(H | X, θ(n)) −
�
H
Lc(θ(n))p(H | X, θ(n))
+
�
H
p(H | X, θ(n)) log p(H | X, θ(n))
p(H | X, θ) .
Note that the left side of the equation remains the same as the variable H
does not occur there. The last term can be recognized as the KL-divergence of
p(H | X, θ(n)) and p(H | X, θ), which is always non-negative. We thus have
L(θ) − L(θ(n)) ≥
�
H
Lc(θ)p(H | X, θ(n)) −
�
H
Lc(θ(n))p(H | X, θ(n))
or
L(θ) ≥
�
H
Lc(θ)p(H | X, θ(n)) + L(θ(n)) −
�
H
Lc(θ(n))p(H | X, θ(n)).
(B.4)
We thus obtain a lower bound for the original likelihood function. The main idea
of EM is to maximize this lower bound so as to maximize the original (incomplete)
likelihood. Note that the last two terms in this lower bound can be treated as
constants as they do not contain the variable θ, so the lower bound is essentially
