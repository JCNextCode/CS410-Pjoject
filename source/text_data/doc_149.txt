Exercises
129
BM25 retrieval function is Robertson and Zaragoza [2009]. A comprehensive survey
of language models for information retrieval can be found in Zhai [2008]. A formal
treatment of retrieval heuristics is given in Fang et al. [2004], and a diagnostic eval-
uation method for assessing deficiencies of a retrieval model is proposed in Fang
et al. [2011], where multiple improved basic retrieval functions are also derived.
Exercises
6.1. Here’s a query and document vector. What is the score for the given document
using dot product similarity?
d = {1, 0, 0, 0, 1, 4}
q = {2, 1, 0, 1, 1, 1}
6.2. In what kinds of queries do we probably not care about query term frequency?
6.3. Let d be a document in a corpus. Suppose we add another copy of d to collec-
tion. How does this affect the IDF of all words in the corpus?
6.4. Given a fixed vocabulary size, the length of a document is the same as the
length of the vector used to represent it. True or false? Why?
6.5. Consider Euclidean distance as our similarity measure for text documents:
d(q, d) =
�
�
�
�
|V |
�
i=1
(qi − di)2.
What does this measure capture compared to the cosine measure discussed in this
chapter? Would you prefer one over the other?
6.6. If you perform stemming on words in V to create V ′ then |V ′| > |V |. True or
false? Why?
6.7. Which of the following ways is best to reduce the size of the vocabulary in a
large corpus?
Remove top 10 words
Remove words that occur 10 times or fewer
6.8. Why might using raw term frequency counts with dot product similarity not
give the best possible ranking?
6.9. How can you apply the VS model to a domain other than text documents? For
example, how do you find similar movies in IMDB or similar music to a specific
song? Hint: first define your concept space; what is your “term” vector?
