370
Chapter 17
Topic Analysis
sports 0.02
game 0.01
basketball 0.005
football 0.004
…
π11
π21 = 0%
πN1 = 0%
π12
π22
πN2
π1k
π2k
πNk
θ1
travel 0.05
attraction 0.03
trip 0.01
…
science 0.04
scientist 0.03
spaceship 0.006
…
θ2
θk
…
…
Text data
Lorem ipsum,
  dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor 
incididunt ut labore et dolore magna aliqua. 
   Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris 
nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor.
   Excepteur sint occaecat cupidatat non proident, sunt in culpa qui 
ofﬁcia deserunt mollit anim id est laborum.
   Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris 
nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in 
reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla 
pariatur.
Lorem ipsum,
  dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor 
incididunt ut labore et dolore magna aliqua. 
   Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris 
nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor.
   Excepteur sint occaecat cupidatat non proident, sunt in culpa qui 
ofﬁcia deserunt mollit anim id est laborum.
   Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris 
nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in 
reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla 
pariatur.
Lorem ipsum,
  dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor 
incididunt ut labore et dolore magna aliqua. 
   Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris 
nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor.
   Excepteur sint occaecat cupidatat non proident, sunt in culpa qui 
ofﬁcia deserunt mollit anim id est laborum.
i
d minim veniam, quis nostrud exercitation ullamco laboris 
i
te irure dolor in
, quis nostrud exercitation ullamco laboris 
si ut aliquip ex ea commodo consequat. Duis aute irure dolor.
   Excepteur sint occaecat cupidatat non proident, sunt in culpa qui 
ofﬁcia deserunt mollit anim id est laborum.
   Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris 
nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in 
reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla 
pariatur.
Lorem ipsum,
  dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor 
incididunt ut labore et dolore magna aliqua. 
   Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris 
nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor.
   Excepteur sint occaecat cupidatat non proident, sunt in culpa qui 
ofﬁcia deserunt mollit anim id est laborum.
   Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris 
nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in 
reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla 
pariatur.
Doc 1
Output: {θ1, …, θk}, {πi1, …, πik}
Input: C, k, V
30%
12%
8%
Doc 2
Doc N
Figure 17.28
Task of mining multiple topics in text.
The formal definition of mining multiple topics from text is illustrated in Fig-
ure 17.28. The input is a collection of text data, the number of topics, and a vocab-
ulary set. The output is of two types. One is topic characterization where each topic
is represented by θi, which is a word distribution. The other is the topic coverage for
each document πij which refers to the probability that document di covers topic θj.
Such a problem can be solved by using PLSA, a generalization of the simple two-
component mixture model to more than two components. Such a more generative
model is illustrated in Figure 17.29, where we also retain the background model
used in the two-component mixture model (which, if you recall, was designed to
discover just one topic). Different from the simple mixture model discussed earlier,
the model here includes k component models, each of which represents a distinct
topic and can be used to generate a word in the observed text data. Adding the
background model θB, we thus have a total of k + 1 component unigram language
models in PLSA.2
2. The original PLSA [Hofmann 1999] did not include a background language model, thus it gives
common words high probabilities in the learned topics if such common words are not removed
in the preprocessing stage.
