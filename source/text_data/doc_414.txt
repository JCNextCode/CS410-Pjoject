394
Chapter 18
Opinion Mining and Sentiment Analysis
the context of the opinion. The only component remaining is to decide the opinion
sentiment of the review. Sentiment classification can be defined more specifically
as follows: the input is an opinionated text object and the output is typically a
sentiment label (or a sentiment tag) that can be defined in two ways. One is polarity
analysis, where we have categories such as positive, negative, or neutral. The other
is emotion analysis that can go beyond polarity to characterize the precise feeling
of the opinion holder. In the case of polarity analysis, we sometimes also have
numerical ratings as you often see in some reviews on the Web. A rating of five
might denote the most positive, and one may be the most negative, for example. In
emotion analysis there are also different ways to design the categories. Some typical
categories are happy, sad, fearful, angry, surprised, and disgusted. Thus, the task
is essentially a classification task, or categorization task, as we’ve seen before.
If we simply apply default classification techniques, the accuracy may not be
good since sentiment classification requires some improvement over regular text
categorization techniques. In particular, it needs two kind of improvements. One
is to use more sophisticated features that may be more appropriate for sentiment
tagging. The other is to consider the order of these categories, especially in polarity
analysis since there is a clear order among the choices. For example, we could use
ordinal regression to predict a value within some range. We’ll discuss this idea in
the next section.
For now, let’s talk about some features that are often very useful for text cat-
egorization and text mining in general, but also especially needed for sentiment
analysis. The simplest feature is character n-grams, i.e., sequences of n adjacent
characters treated as a unit. This is a very general and robust way to represent text
data since we can use this method for any language. This is also robust to spelling
errors or recognition errors; if you misspell a word by one character, this representa-
tion still allows you to match the word as well as when it occurs in the text correctly.
Of course, such a representation would not be as discriminating as words.
Next, we have word n-grams, a sequence of words as opposed to characters. We
can have a mix of these with different n-values. Unigrams are often very effective for
text processing tasks; it’s mostly because words are the basic unit of information
used by humans for communication. However, unigram words may not be suffi-
cient for a task like sentiment analysis. For example, we might see a sentence, “It’s
not good” or “It’s not as good as something else.” In such a case, if we just take
the feature good, that would suggest a positive text sample. Clearly, this would not
be accurate. If we take a bigram (n = 2) representation, the bigram not good would
appear, making our representation more accurate. Thus, longer n-grams are gen-
erally more discriminative. However, long n-grams may cause overfitting because
