474
Appendix C
KL-divergence and Dirichlet Prior Smoothing
Note that the second term on the right-hand side of the formula is a query-
dependent constant, or more specifically, the entropy of the query model �θQ. It can
be ignored for the purpose of ranking documents. In general, the computation of
the above formula involves a sum over all the words that have a non-zero probability
according to p(w | �θQ). However, when �θD is based on certain general smoothing
method, the computation would only involve a sum over those that both have a
non-zero probability according to p(w | �θQ) and occur in document d. Such a sum
can be computed much more efficiently with an inverted index.
We now explain this in detail. The general smoothing scheme we assume is the
following:
p(w | �θD) =
�
ps(w | d)
if word w is seen
αdp(w | C)
otherwise.
whereps(w | d)isthesmoothedprobabilityofawordseeninthedocument, p(w | C)
is the collection language model, and αd is a coefficient controlling the probability
mass assigned to unseen words, so that all probabilities sum to one. In general, αd
may depend on d. Indeed, if ps(w | d) is given, we must have
α =
1 − �
w:c(w;d)>0 ps(w | d)
1 − �
w:c(w;d)>0 p(w | C) .
Thus, individual smoothing methods essentially differ in their choice of ps(w | d).
The collection language model p(w | C) is typically estimated by
c(w,C)
�
w′ c(w′,C), or a
smoothed version
c(w,C)+1
V +�
w′ c(w′,C), where V is an estimated vocabulary size (e.g., the
total number of distinct words in the collection). One advantage of the smoothed
version is that it would never give a zero probability to any term, but in terms
of retrieval performance, there will not be any significant difference in these two
versions, since �
w′ c(w′, C) is often significantly larger than V .
It can be shown that with such a smoothing scheme, the KL-divergence scoring
formula is essentially (the two sides are equivalent for ranking documents)
�
w
p(w | �θQ) log p(w | �θD) =
⎡
⎢⎣
�
w:c(w;d)>0,p(w|�
θQ)>0
p(w | �θQ) log ps(w | d)
αdp(w | C)
⎤
⎥⎦
+ log αd.
(C.1)
Note that the scoring is now based on a sum over all the terms that both have
a non-zero probability according to p(w | �θQ) and occur in the document, i.e., all
“matched” terms.
