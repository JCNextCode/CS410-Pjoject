17.1 Topics as Terms
333
very frequently in English, so we also want to avoid having such words on the top.
That is, we would like to favor terms that are fairly frequent but not too frequent.
A specific approach to achieving our goal is to use TF-IDF weighting discussed
in some previous chapters of the book on retrieval models and word association
discovery. An advantage of using such a statistical approach to define a scoring
function is that the scoring function would be very general and can be applied
to any natural language, any text. Of course, when we apply such an approach
to a particular problem, we should always try to leverage some domain-specific
heuristics. For example, in news we might favor title words because the authors
tend to use the title to describe the topic of an article. If we’re dealing with tweets,
we could also favor hashtags, which are invented to denote topics.
After we have designed the scoring function, we can discover the k topical terms
by simply picking the k terms with the highest scores. We might encounter a
situation where the highest scored terms are all very similar. That is, they are
semantically similar, or closely related, or even synonyms. This is not desirable
since we also want to have a good coverage over all the content in the collection,
meaning that we would like to remove redundancy. One way to do that is to use a
greedy algorithm, called Maximal Marginal Relevance (MMR) re-ranking.
The idea is to go down the list based on our scoring function and select k topical
terms. The first term, of course, will be picked. When we pick the next term, we will
look at what terms have already been picked and try to avoid picking a term that’s
too similar. So while we are considering the ranking of a term in the list, we are
also considering the redundancy of the candidate term with respect to the terms
that we already picked. With appropriate thresholding, we can then get a balance
of redundancy removal and picking terms with high scores. The MMR technique is
described in more detail in Chapter 16.
After we obtain k topical terms to denote our topics, the next question is how to
compute the coverage of each topic in each document, πij. One solution is to simply
count occurrences of each topical term as shown in Figure 17.5. So, for example,
sports might have occurred four times in document di, and travel occurred twice.
We can then just normalize these counts as our estimate of the coverage probability
for each topic. The normalization is to ensure that the coverage of each topic in the
document would add to one, thus forming a distribution over the topics for each
document to characterize coverage.
As always, when we think about an idea for solving a problem, we have to ask the
following questions: how effective is the solution? Is this the best way of solving
problem? In general, we have to do some empirical evaluation by using actual data
sets and to see how well it works. However, it is often also instructive to analyze
