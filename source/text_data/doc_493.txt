C
APPENDIX
KL-divergence
and Dirichlet
Prior Smoothing
This appendix is a more detailed discussion of the KL-divergence function and its
relation to Dirichlet prior smoothing in the generalized query likelihood smoothing
framework. We briefly touched upon KL-divergence in Chapter 7 and Chapter 13.
As we have seen, given two probability mass functions p(x) and q(x), D(p∥q),
the Kullback-Leibler divergence (or relative entropy) between p and q is defined as
D(p∥q) =
�
x
p(x) log p(x)
q(x) .
It is easy to show that D(p∥q) is always non-negative and is zero if and only if
p = q. Even though it is not a true distance between distributions (because it is
not symmetric and does not satisfy the triangle inequality), it is still often useful
to think of the KL-divergence as a “distance” between distributions [Cover and
Thomas 1991].
C.1
Using KL-divergence for Retrieval
Suppose that a query q is generated by a generative model p(q | θQ) with θQ denot-
ing the parameters of the query unigram language model. Similarly, assume that a
document d is generated by a generative model p(d | θD) with θD denoting the pa-
rameters of the document unigram language model. If �θQ and �θD are the estimated
query and document language models, respectively, then, the relevance value of d
with respect to q can be measured by the following negative KL-divergence function
[Zhai and Lafferty 2001]:
−D(�θQ∥�θD) =
�
w
p(w | �θQ) log p(w | �θD) + (−
�
w
p(w | �θQ) log p(w | �θQ)).
