348
Chapter 17
Topic Analysis
P (“the”) = p(θd)p(“the” | θd) + p(θB)p(“the” | θB)
= 0.5 × 0.000001 + 0.5 × 0.03
P (“text”) = p(θd)p(“text” | θd) + p(θB)p(“text” | θB)
= 0.5 × 0.04 + 0.5 × 0.000006
Figure 17.14
Probability of the and text.
a word from the background language model p(w | θB). Thus, the probability of
observing the from the background model is p(θB)p(the | θB), and the probability
of observing the from the mixture model regardless of which distribution we use
would be p(θB)p(the | θB) + p(θd)p(the | θd), as shown in Figure 17.14, where we
also show how to compute the probability of text.
It is not hard to generalize the calculation to compute the probability of observ-
ing any word w from such a mixture model, which would be
p(w) = p(θB)p(w | θB) + p(w | θd)p(θd).
(17.4)
The sum is over the two different ways to generate the word, corresponding to us-
ing each of the two distributions. Each term in the sum captures the probability of
observing the word from one of the two distributions. For example, p(θB)p(w | θB)
gives the probability of observing word w from the background language model.
The product is due to the fact that in order to observe word w, we must have (1) de-
cided to use the background distribution (which has the probability of p(θB)), and
(2)obtainedwordw fromthedistributionθB (whichhastheprobabilityofp(w | θB)).
Both events must happen in order to observe word w from the background distri-
bution, thus we multiply their probabilities to obtain the probability of observing
w from the background distribution. Similarly, p(θd)p(w | θd) gives the probability
of observing word w from the topic word distribution. Adding them together gives
us the total probability of observing w regardless which distribution has actually
been used to generate the word.
Such a form of likelihood actually reflects some general characteristics of the
likelihood function of any mixture model. First, the probability of observing a
data point from a mixture model is a sum over different ways of generating the
word, each corresponding to using a different component model in the mixture
model. Second, each term in the sum is a product of two probabilities: one is the
probability of selecting the component model corresponding to the term, while
