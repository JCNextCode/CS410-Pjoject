362
Chapter 17
Topic Analysis
updated by incorporating the data likelihood p(text | θd) and p(text | θB) so that we
would favor a distribution that gives text a higher probability.
In the example shown in Figure 17.23, our prior says that each of the two models
is equally likely; thus, it is a non-informative prior (one with no bias). As a result, our
inference of which distribution has been used to generate a word would solely be
based on p(w | θd) and p(w | θB). Since p(text | θd) is much larger than p(text | θB),
we can conclude that θd is much more likely the distribution that has been used to
generate text. In general, our prior may be biased toward a particular distribution.
Indeed, a heavily biased prior can even dominate over the data likelihood to essen-
tially dictate the decision. For example, imagine our prior says p(θB) = 0.99999999,
then our inference result would say that text is more likely generated by θB than by
θd even though p(text | θd) is much higher than p(text | θB), due to the very strong
prior. Bayes’ Rule provides us a principled way of combining the prior and data
likelihood.
In Figure 17.23, we introduced a binary latent variable z here to denote whether
the word is from the background or the topic. When z is 0, it means it’s from
the topic, θd; when it’s 1, it means it’s from the background, θB. The posterior
probability p(z = 0 | w = text) formally captures our guess about which distribution
has been used to generate the word text, and it is seen to be proportional to the
product of the prior p(θd) and the likelihood p(text | θd), which is intuitively very
meaningful since in order to generate text from θd, we must first choose θd (as
opposed to θB), which is captured by p(θd), and then obtain word text from the
selected θd, which is captured by p(w | θd).
Understanding how to make such a Bayesian inference of which distribution has
been used to generate a word based on a set of tentative parameter values is very
crucial for understanding the EM algorithm. This is essentially the E-step of the
EM algorithm where we use Bayes’ rule to partition data and allocate all the data
points among all the component models in the mixture model.
Note that the E-step essentially helped us figure out which words have been
generated from θd (and equivalently, which words have been generated from θB)
except that it does not completely allocate a word to θd (or θB), but splits a word in
between the two distributions. That is, p(z = 0 | text) tells us what percent of the
count of text should be allocated to θd, and thus contribute to the estimate of θd.
This way, we will be able to collect all the counts allocated to θd, and renormalize
them to obtain a potentially improved estimate of p(w | θd), which is our goal. This
step of re-estimating parameters based on the results from the E-step is called the
M-step.
