360
Chapter 17
Topic Analysis
θd
θB
p(w|θd)
p(θd) = 0.5
p(θd) + p(θB) = 1
p(θB) = 0.5
p(w|θB)
text ?
mining ?
association ?
clustering ?
…
the ?
the 0.03
a 0.02
is 0.015
we 0.01
food 0.003
…
text 0.000006
Topic choice
d
d′
… text mining …
is … clustering …
we … Text … the
If we know which word is from which distribution …
p(wi|θd) = 
c(w′, d′)
c(wi, d′) 
∑
w′2V
Figure 17.22
Estimation of a topic when each word is known to be from a particular distribution.
the background model. The other group will be explained by the unknown topical
model (the topic word distribution). The challenge in computing the ML estimate
is that we do not know this partition due to the possibility of generating a word
using either of the two distributions in the mixture model.
If we actually know which word is from which distribution, computation of the
ML estimate would be trivial as illustrated in Figure 17.22, where d′ is used to denote
the pseudo document that is composed of all the words in document d that are
known to be generated by θd, and the ML estimate of θd is seen to be simply the
normalized word frequency in this pseudo document d′. That is, we can simply pool
together all the words generated from θd, compute the count of each word, and then
normalize the count by the total counts of all the words in such a pseudo document.
In such a case, our mixture model is really just two independent unigram language
models, which can thus be estimated separately based on the data points generated
by each of them.
Unfortunately, the real situation is such that we don’t really know which word
is from which distribution. The main idea of the EM algorithm is to guess (infer)
which word is from which distribution based on a tentative estimate of parameters,
and then use the inferred partitioning of words to improve the estimate of param-
eters, which, in turn, enables improved inference of the partitioning, leading to an
iterative hill-climbing algorithm to improve the estimate of the parameters until
hitting a local maximum. In each iteration, it would invoke an E-step followed by
an M-step, which will be explained in more detail.
