366
Chapter 17
Topic Analysis
column under Iteration 2). Those words that are believed to have come from the
topic word distribution θd according to the E-step would have a higher probability.
This new generation of parameters would allow us to further adjust the inferred
latent variable or hidden variable values, leading to a new generation of probabili-
ties for the z values, which can be fed into another M-step to generate yet another
generation of potentially improved estimate of θd.
In the last row of the table, we show the log-likelihood after each iteration. Since
each iteration would lead to a different generation of parameter estimates, it would
also give a different value for the log-likelihood function. These log-likelihood val-
ues are all negative because the probability is between 0 and 1, which becomes a
negative value after the logarithm transformation. We see that after each iteration,
the log-likelihood value is increasing, showing that the EM algorithm is iteratively
improving the estimated parameter values in a hill-climbing manner. We will pro-
vide an intuitive explanation of why it converges to a local maximum later.
For now, it is worth pointing out that while the main goal of our EM algorithm
is to obtain a more discriminative word distribution to represent the topic that we
hope to discover, i.e., p(w | θd), the inferred p(z = 0 | w) after convergence is also
meaningful and may sometimes be a useful byproduct. Specifically, these are the
probabilities that a word is believed to have come from the topic distribution, and
we can add them up to obtain an estimate of to what extent the document has cov-
ered background vs. content, or to what extent the content of the document deviates
from a “typical” background document. This would give us a single numerical score
for each document, so we can then use the score to compare different documents
or different subsets of documents (e.g., those associated with different authors or
from different sources). Thus, our simple two-component mixture model can not
only help us discover a single topic from the document, but also provide a useful
measure of “typicality” of a document which may be useful in some applications.
Next, we provide some intuitive explanation why the EM algorithm will converge
to a local maximum in Figure 17.26. Here we show the parameter θd on the X-axis,
and the Y-axis denotes the likelihood function value. This is an over-simplification
since θd is an M-dimensional vector, but the one-dimensional view makes it much
easier to understand the EM algorithm. We see that, in general, the original like-
lihood function (as a function of θd) may have multiple local maxima. The goal of
computing the ML estimate is to find the global maximum, i.e., the θd value that
makes the likelihood function reach it global maximum.
The EM algorithm is a hill-climbing algorithm. It starts with an initial (random)
guess of the optimal parameter value, and then iteratively improves it. The picture
shows the scenario of going from iteration n to iteration n + 1. At iteration n, the
