17.3 Mining One Topic from Text
349
the other is the probability of actually observing the data point from that selected
component model. Their product gives the probability of observing the data point
when it is generated using the corresponding component model, which is why the
sum would give the total probability of observing the data point regardless which
component model has been used to generate the data point. As will be seen later,
more sophisticated topic models tend to use more than two components, and their
probability of generating a word would be of the same form as we see here except
that there are more than two products in the sum (more precisely as many products
as the number of component models).
Once we write down the likelihood function for one word, it is very easy to see that
as a whole, the mixture model can be regarded as a single word distribution defined
in a somewhat complicated way. That is, it also gives us a probability distribution
over words as defined above. Thus, conceptually the mixture model is yet another
generative model that also generates a sequence of words by generating each word
independently. This is the same as the case of a simple unigram language model,
which defines a distribution over words by explicitly specifying the probability of
each word.
The main idea of a mixture model is to group multiple distributions together as
one model, as shown in Figure 17.15, where we draw a box to “encapsulate” the two
distributions to form a single generative model. When viewing the whole box as one
model, we can easily see that it’s just like any other generative model that would
give us the probability of each word. However, how this probability is determined
in such a mixture model is quite different from when we have just one unigram
language model.
w
θd
θB
p(w|θd)
p(θd) = 0.5
p(θd) + p(θB) = 1
p(θB) = 0.5
p(w|θB)
text 0.04
mining 0.035
association 0.03
clustering 0.005
…
the 0.000001
the 0.03
a 0.02
is 0.015
we 0.01
food 0.003
…
text 0.000006
Topic choice
Mixture model
“the”?
“text”?
Figure 17.15
The idea of a mixture language model.
