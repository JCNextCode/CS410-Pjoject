15.5 Classification Algorithms
307
advanced techniques comes a requirement for more processing time. When using
features from grammatical parse trees, a parser must first be run across the data-
set, which is often at least an order of magnitude slower than simple whitespace-
delimited unigram words processing. Running a parser requires the sentence to
be part-of-speech tagged, and running coreference resolution requires grammat-
ical parse trees. The level of sophistication in the syntactic or semantic features
usually depends on the practitioner’s tolerance for processing time and memory
usage.
15.5
Classification Algorithms
In this section, we will look into how the function f (.) is actually able to distinguish
between class labels. We examine three different algorithms, all of which are avail-
able in META. We will continue to use the sentiment analysis example, classifying
new text into either the positive or negative label. Let’s also assume we split our cor-
pus into a training partition and testing partition. The training documents will be
used to build f (.), and we will be able to evaluate its performance on each testing
document.
Remember that we additionally have the metadata information Y for all docu-
ments, so we know the true labels of all the testing and training data. When used in
production, we will not know the true label (unless a human assigns one), but we
can have some confidence of the algorithm’s prediction based on its performance
on the testing data, which mimics the unseen data from the real world. The closer
the testing data is to the data you expect to see in the wild, the greater is your belief
in the classifier’s accuracy.
15.5.1
k-Nearest Neighbors
k-NN is a learning algorithm that directly uses our inverted index and search engine.
Unlike the next two algorithms we will discuss, there is no explicit training step; all
we need to do is index the training documents. This makes k-NN a lazy learner or
instance-based classifier.
As shown in the training and testing algorithms, the basic idea behind k-NN is to
find the most similar documents to the query document, and use the most common
class label of the similar documents. The assumption is that similar documents
will have the same class label. Figure 15.3 shows an example of k-NN in action in
the document vector space. Here there are three different classes represented as
different colors plotted in the vector space. If k = 1, we would assign the red label
to the query; if k = 4, we would assign the blue label, since three out of the top four
