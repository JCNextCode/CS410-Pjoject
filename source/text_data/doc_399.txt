17.5 Extension of PLSA and Latent Dirichlet Allocation
379
= 
p(n+1)(w|θj) = 
EM Algorithm with Conjugate on p(w|θj)
Prior: p(w|θ′j)
battery 0.5
life 0.5
What if μ = 0? What if μ = +∞?
Sum of all pseudo counts
Pseudo counts
of w from
prior θ′
∑w2V c(w, d)(1 – p(zd,w = B))p(zd,w = j)
—
∑ j′∑w2Vc(w, d)(1 – p(zd,w = B))p(zd,w = j′)
∑d2C c(w, d)(1 – p(zd,w = B))p(zd,w = j) + μp(w|θ′j)
—
∑w′2V∑d2Cc(w′, d)(1 – p(zd,w′ = B))p(zd,w′ = j) + μ
p(zd,w = B) = 
λBp(w|θB)
—
λBp(w|θB) + (1 – λB)∑k
j=1π(
d
n
,
)
jp(n)(w|θj)
p(zd,w = j) = 
π(
d
n
,
)
jp(n)(w|θj)
—
∑k
j′=1π(
d
n
,
)
j′p(n)(w|θj′)
π(n+1)
d,j
Figure 17.35
Maximum a posteriori estimation of PLSA with prior.
representing the topics p(w | θj), then the EM algorithm for computing the MAP is
shown in Figure 17.35. We see that the difference is adding an additional pseudo
count for word w in the M-step which is proportional to the probability of the word
in the prior p(w | θ′
j). Specifically, the pseudo count is μp(w | θ′
j) for word w. The
denominator needs to be adjusted accordingly (adding μ which is the sum of all
the pseudo counts for all the words) to ensure the estimated word probabilities for
a topic sum to one.
Here, μ ∈ [0, +∞) is a parameter encoding the strength of our prior. If μ = 0,
we recover the original EM algorithm for PLSA, i.e., with no prior influence. A
more interesting case is when μ = +∞, in such a case, the M-step is simply to set
the estimated probability of a word p(w | θj) to the prior p(w | θ′
j), i.e., the word
distribution is fixed to the prior. This is why we can interpret our heuristic inclusion
of a background word distribution as a topic in PLSA as simply imposing such an
infinitely strong prior on one of the topics. Intuitively, in Bayesian inference, this
means that if the prior is infinitely strong, then no matter how much data we collect,
we will not be able to override the prior. In general, however, as we increase the
amount of data, we will be able to let the data dominate the estimate, eventually
overriding the prior completely as we collect infinitely more data. A prior on the
