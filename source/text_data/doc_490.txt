470
Appendix B
Expectation- Maximization
p(zij = 1 | F , θ(n)
F ) =
λp(dij | C)
λp(dij | C) + (1 − λ)p(dij | θ(n)
F )
.
(B.5)
And, of course, p(zij = 0 | F , θ(n)
F ) = 1 − p(zij = 1 | F , θ(n)
F ). Note that, in general, zij
may depend on all the words in F. In our model, however, it only depends on the
corresponding word dij.
The M-step involves maximizing the Q-function. This may sometimes be quite
complex as well. But, again, in our case, we can find an analytical solution. In order
to achieve this, we use the Lagrange multiplier method since we have the following
constraint on the parameter variables {p(w | θF)}w∈V , where V is our vocabulary:
�
w∈V
p(w | θF) = 1.
We thus consider the following auxiliary function:
g(θF) = Q(θF; θ(n)
F ) + μ(1 −
�
w∈V
p(w | θF))
and take its derivative with respect to each parameter variable p(w | θF)
∂g(θF)
∂p(w | θF) =
⎡
⎣
k
�
i=1
|di|
�
j=1,dij=w
p(zij = 0 | F , θ(n)
F )
p(w | θF)
⎤
⎦ − μ.
(B.6)
Setting this derivative to zero and solving the equation for p(w | θF), we obtain
p(w | θF) =
�k
i=1
�|di|
j=1,dij=w p(zij = 0 | F , θ(n)
F )
�k
i=1
�|di|
j=1 p(zij = 0 | F , θ(n)
F )
(B.7)
=
�k
i=1 p(zw = 0 | F , θ(n)
F )c(w, di)
�k
i=1
�
w′∈V p(zw′ = 0 | F , θ(n)
F )c(w′, di)
.
(B.8)
Note that we changed the notation so that the sum over each word position in
document di is now a sum over all the distinct words in the vocabulary. This is
possible, because p(zij | F , θ(n)
F ) depends only on the corresponding word dij. Using
word w, rather then the word occurrence dij, to index z, we have
p(zw = 1 | F , θ(n)
F ) =
λp(w | C)
λp(w | C) + (1 − λ)p(w | θ(n)
F )
.
(B.9)
We therefore have the following EM updating formulas for our simple mixture
model:
