7.2 Feedback in Language Models
143
we only have word probabilities θ as parameters, just like in the simplest unigram
language model. This gives us the following formula to estimate the feedback lan-
guage model:
θF = arg max
θ
log p(F | θ)
= arg max
θ
�
d∈F
�
w
c(w, d) . log [(1 − λ) . p(w | θ) + λ . p(w | C)]
(7.2)
We choose this probability distribution θF to maximize the log likelihood of
the feedback documents under our model. This is the same idea as the maximum
likelihood estimator. Here though, the mathematical problem is to solve this opti-
mization problem. We could try all possible θ values and select the one that gives
the whole expression the maximum probability. Once we have done that, we ob-
tain this θF that can be interpolated with the original query model to do feedback.
Of course, in practice it isn’t feasible to try all values of θ, so we use the EM algo-
rithm to estimate its parameters [Zhai and Lafferty 2001]. Such a model involving
multiple component models combined together is called a mixture model, and
we will further discuss such models in more detail in the topic analysis chapter
(Chapter 17).
Figure 7.6 shows some examples of the feedback model learned from a web docu-
mentcollectionforperformingpseudofeedback.Wejustusethetop10documents,
and we use the mixture model with parameters λ = 0.9 and λ = 0.7. The query is air-
port security. We select the top ten documents returned by the search engine for this
query and feed them to the mixture model. The words in the two tables are learned
using the approach we described. For example, the words airport and security still
show up as high probabilities in each case naturally because they occur frequently
in the top-ranked documents. But we also see beverage, alcohol, bomb, and terrorist.
Clearly, these are relevant to this topic, and if combined with the original query can
help us match other documents in the index more accurately.
If we compare the two tables, we see that when λ is set to a smaller value, we’ll
still see some common words when we don’t use the background model often.
Remember that λ can “choose” the probability of using the background model to
generate to the text. If we don’t rely much on the background model, we still have
to use the topic model to account for the common words. Setting λ to a very high
value uses the background model more often to explain these words and there is no
burden on explaining the common words in the feedback documents. As a result,
the topic model is very discriminative—it contains all the relevant words without
common words.
