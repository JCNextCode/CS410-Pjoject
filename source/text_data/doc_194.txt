174
Chapter 9
Search Engine Evaluation
9.3
Evaluation of a Ranked List
In the previous section, we only considered whether a relevant document appeared
in the results or not—a binary measure. In this section, we will see how we can take
each document’s position into account when assigning an evaluation score.
We saw that precision and recall are the two basic ways to quantitatively measure
theperformanceofasearchresult.But, aswetalkedaboutindepthinChapter5, the
text retrieval problem is a ranking problem, not a classification one. Thus, we need
to evaluate the quality of a ranked list as opposed to whether a relevant document
was returned anywhere in the results.
How can we use precision and recall to evaluate a ranked list? Naturally, we will
have to look at precision and recall at different cutoffs since a ranked list of relevant
documents is determined by where the user stops browsing. If we assume the user
sequentially browses the list of results, the user would stop at some point. That
point would determine the size of the set. Therefore, that’s the most important
cutoff that we have to consider when we compute the precision-recall.
Without knowing where exactly the user would stop, we have to consider all the
possible positions where they might stop. A precision-recall curve does exactly this,
as illustrated in Figure 9.4
What if the user stops at the first document? What’s the precision-recall at this
point? Since D1 is relevant, the precision is one out of one since we have one
Total number of relevant documents in collection: 10
Evaluating Ranking: Precision–Recall (PR) Curve
0.1 0.2
1.0
0.8
0.6
0.4
0.2
0.0
D1 +
D2 +
D3 –
D4 –
D5 +
D6 –
D7 –
D8 +
D9 –
D10 –
1/1
2/2
2/3
3/5
4/8
?
1/10
2/10
2/10
3/10
4/10
10/10
Precision
Recall
0.3
1.0
0.4
Precision
Recall
…
Figure 9.4
Computing a precision-recall curve.
