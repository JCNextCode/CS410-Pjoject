17.3 Mining One Topic from Text
347
assign high probabilities to such content-bearing words rather than the common
function words in English.
The assumed process for generating a word with such a mixture model is just
slightly different from the generation process of our simplest unigram language.
Since we now have two distributions, we have to decide which distribution to
use when we generate the word, but each word will still be a sample from one
of the two distributions. The text data are still generated in the same way, by
generating one word at a time. More specifically, when we generate a word, we
first decide which of the two distributions to use. This is controlled by a new
probability distribution over the choices of the component models to use (two
choices in our case), including specifically the probability of θd (using the unknown
topic model) and the probability of θB (using the known background model). Thus,
p(θd) + p(θB) = 1.
In the figure, we see that both p(θd) and p(θB) are set to 0.5. This means that we
can imagine flipping a fair coin to decide which distribution to use, although in
general these probabilities don’t have to be equal; one topic could be more likely
than another.
The process of generating a word from such a mixture model is as follows. First,
we flip a biased coin which would show up as heads with probability p(θd) (and
thus as tails with probability p(θB) = 1 − p(θd)) to decide which word distribution
to use. If the coin shows up as heads, we would use θd; otherwise, θB. We will use
the chosen word distribution to generate a word. This means if we are to use θd,
we would sample a word using p(w | θd), otherwise using p(w | θB), as illustrated in
Figure 17.13.
We now have a generative model that has some uncertainty associated with the
use of which word distribution to generate a word. If we treat the whole generative
model as a black box, the model would behave very similarly to our simplest topic
model where we only use one word distribution in that the model would specify
a distribution over sequences of words. We can thus examine the probability of
observing any particular word from such a mixture model, and compute the prob-
ability of observing a sequence of words.
Let’sassumethatwehaveamixturemodelasshowninFigure17.13andconsider
two specific words, the and text. What’s the probability of observing a word like
the from the mixture model? Note that there are two ways to generate the, so the
probability is intuitively a sum of the probability of observing the in each case.
What’s the probability of observing the being generated using the background
model? In order for the to be generated in this way, we must have first chosen
to use the background model, and then obtained the word the when sampling
