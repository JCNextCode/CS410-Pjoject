9.3 Evaluation of a Ranked List
175
document and it is relevant. What about the recall? Note that we are assuming
that there are ten relevant documents for this query in the collection so it’s one out
of ten.
What if the user stops at the second position? The precision is the same since
both D1 and D2 are relevant: 100%, or two out of two. The recall is two out of
ten, or 20%. If the user stops at the third position, we have an interesting case
because we don’t have any additional relevant documents, so the recall does not
change. However, the precision is lower because we have two out of three relevant
documents. The recall won’t change until we see another relevant document. In
this case, that point is at D5. There, the recall has increased to three out of ten and
the precision is three out of five. As you can see, if we keep doing this, we can also get
to D8 and have a precision of four out of eight, because there are eight documents
and four of them are relevant. There, the recall is four out of ten.
When can we get a recall of five out of ten? In this list, we don’t have it. For
convenience, we often assume that the precision is zero in a situation like this.
This is a pessimistic assumption since the actual precision would be higher, but
we make this assumption in order to have an easy way to compute another measure
called average precision, that we will discuss soon.
Note that we’ve made some assumptions that are clearly not accurate. But, this
is okay for the relative comparison of two text retrieval methods. As long as the
deviation is not biased toward any particular retrieval method, the measure is
acceptable since we can still accurately tell which method works better. This is the
most important point to keep in mind: when you compare different algorithms,
the key is to avoid any bias toward a particular method. As long as you can avoid
that, it’s perfectly fine to do a transformation of these measures that preserves the
order.
Since we can get a lot of precision-recall numbers at different positions, we can
plot a curve; this is what’s shown on the right side of Figure 9.4. On the x-axis
are the recall values. On the y-axis are the precision values. We plot precision-
recall numbers so that we display at what recall we can obtain a certain precision.
Furthermore, we can link these points to form a curve. As you see in the figure, we
assumed all the precision values at the high-level recalls are zero. Although the real
curves will not be exactly like this, it doesn’t matter that much for comparing two
methods whether we get the exact precision values here or not.
In Figure 9.5, we compare two systems by plotting their PR-curves on the same
graph. System A is shown in red and system B is shown in blue. Which one is better?
On the left, system A is clearly better since for the same level of recall, the precision
value by system A is better than system B. In general, the higher the curve is, the
