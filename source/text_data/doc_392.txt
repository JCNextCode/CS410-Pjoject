372
Chapter 17
Topic Analysis
Percentage of
background
words (known)
Probability of word w in topic θj
Background
LM (known)
Coverage of topic θj in doc d
pd(w) = λB p(w|θB) + (1 – λB) 
πd,jp(w|θj)
k
∑
j=1
k
∑
j=1
log p(d) =  
Unknown parameters: � = ({πd,j}, {θj}), j = 1, …, k
Probabilistic Latent Semantic Analysis (PLSA)
c(w, d) log [λB p(w|θB) + (1 – λB) 
πd,jp(w|θj)]
∑
w2V
k
∑
j=1
log p(C|�) =  
c(w, d) log [λB p(w|θB) + (1 – λB) 
πd,jp(w|θj)]
∑
w2V
∑
d2C
Figure 17.30
The likelihood function of PLSA.
of selecting the particular component model and the probability of observing the
particular word from the particular selected word distribution.
The likelihood function is as illustrated in Figure 17.30. Specifically, the prob-
ability of observing a word from the background distribution is λBp(w | θB), while
the probability of observing a word from a topic θj is (1 − λB)πd,jp(w | θj). The
probability of observing the word regardless of which distribution is used, pd(w),
is just a sum of all these cases.
Assuming that the words in a document are generated independently, it follows
that the likelihood function for document d is the second equation in Figure 17.30,
and that the likelihood function for the entire collection C is given by the third
equation.
What are the parameters in PLSA? First, we see λB, which represents the percent-
ageofbackgroundwordsthatwebelieveexistinthetextdata(andthatwewouldlike
to factor out). This parameter can be set empirically to control the desired discrim-
ination of the discovered topic models. Second, we see the background language
model p(w | θB), which we also assume is known. We can use any large collection
of text, or use all the text that we have available in collection C to estimate p(w | θB)
(e.g., assuming all the text data are generated from θB, we can use the ML estimate
to set p(w | θB) to the normalized count of word w in the data). Third, we see πd,j,
which indicates the coverage of topic θj in document d. This parameter encodes the
knowledge we hope to discover from text. Finally, we see the k word distributions,
