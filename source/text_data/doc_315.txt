14.4 Evaluation of Text Clustering
295
same or different clusters. Even if your clustering algorithm performs spectacularly
in terms of (for example) intra-cluster similarity, the clusters may not be acceptable
from a human viewpoint unless an adequate feature representation was used; it’s
possible that the feature representation is not able to capture a crucial concept and
needs to be reexamined. Chapter 4 gives a good overview of many different textual
features supported by META. In the next chapter on text categorization (Chapter 15),
we also discuss how choosing the right features plays an important role in the over-
all classification accuracy.
As we saw in this chapter, similarity-based algorithms explicitly encode a simi-
larity function in their implementation. Ideally, this similarity between objects is
optimized to maximize intra-cluster coherence and minimize intra-cluster separa-
tion. In model-based methods (which will be discussed in Chapter 17), similarity
functions are not inherently part of the model; instead, the notion of object sim-
ilarity is most often captured by probabilistically high co-occurring terms within
“similar” objects.
Measuring coherence and separation automatically can potentially be accom-
plished by leveraging a categorization data set; such a corpus has predefined clus-
ters where each document belongs to a particular category. For example, a text
categorization corpus could be product descriptions from an online retailer, and
each product belongs in a product category, such as kitchenware, books, grocery,
and so on. A clustering algorithm would be effective if it was able to partition
the products based on their text into categories that roughly matched the prede-
fined ones. A simple measure to evaluate this application would be to consider
each output cluster and see if one of the predefined categories dominates the clus-
ter population. In other words, take each cluster Ci and calculate the percentage
of each predefined class in it. The clustering algorithm would be effective if, for
each Ci, one predefined category dominates and scarcely appears in other clusters.
Effectively, the clustering algorithm recreated the class assignments in the origi-
nal dataset without any supervision. Of course, however, we have to be careful (if
this is a parameter), to set the final number of clusters to match the number of
classes.
In fact, deciding the optimal number of clusters is a hard problem for all meth-
ods! For example, in K-means, the final clusters depend on the initial random
starting positions. Thus it’s quite common to run the algorithm several times and
manually inspect the results. The algorithm G-means [Hamerly and Elkan 2003]
reruns K-means in a more principled way, splitting clusters if the data assigned to
each cluster is not normally-distributed. Model-based methods may have some ad-
vantages in terms of deciding the optimal number of clusters, but the model itself
