350
Chapter 17
Topic Analysis
It’s often useful to examine some special cases of a model as such an exercise can
help interpret the model intuitively and reveal relations between simpler models
and a more complicated model. In this case, we can examine what would happen
if we set the probability of choosing the background component model to zero. It
is easy to see that in such a case, the term corresponding to the background model
would disappear from the sum, and the mixture model would degenerate to the
special case of just one distribution characterizing the topic to be discovered. In
this sense, the mixture model is more general than the previous model where we
have just one distribution, which can be covered as a special case. Naturally, our
reason for using a mixture model is to enforce a non-zero probability of choosing
the background language model so that it can help explain the common words in
the data and allow our topic word distribution to be more concentrated on content
words.
Once we write down the likelihood function, the next question is how to esti-
mate the parameters. As in the case of the single unigram language model, we
can use any method (e.g., the maximum likelihood estimator) to estimate the pa-
rameters, which can then be regarded as the knowledge that we discover from
the text.
.
Data: Document d
.
Mixture Model: parameters � = ({p(w | θd)}, {p(w | θB)}, p(θB), p(θd))
Two unigram LMs: θd (the topic of d); θB (background topic)
Mixing weight (topic choice): p(θd) + p(θB) = 1
.
Likelihood function:
p(d | �) =
|d|
�
i=1
p(xi | �) =
|d|
�
i=1
[p(θd)p(xi | θd) + p(θB)p(xi | θB)]
=
M
�
i=1
[p(θd)p(wi | θd) + p(θB)p(wi | θB)]c(w,d)
.
ML Estimate: �∗ = arg max� p(d | �)
Subject to
M
�
i=1
p(wi | θd) =
M
�
i=1
p(wi | θB) = 1
p(θd) + p(θB) = 1
Figure 17.16
Summary of a two-component mixture model.
