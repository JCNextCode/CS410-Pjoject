460
Appendix A
Bayesian Statistics
This can be accomplished with α = 4, β = 1, or α = 16, β = 4, or even α = 0.4, β =
0.1. But what is the difference? Figure A.1 shows a comparison of the Beta distribu-
tion with varying parameters. It’s also important to remember that a draw from a
Beta prior θ ∼ Beta(α, β) gives us a distribution. Even though it’s a single value on
the range [0, 1], we are still using the prior to produce a probability distribution.
Perhaps we’d like to choose a unimodal Beta prior, with a mean 0.8. As we can
see from Figure A.1, the higher we set α and β, the sharper the peak at 0.8 will be.
Looking at our parameter estimation,
H + α
H + T + α + β ,
(A.10)
we can imagine the hyperparameters as pseudo counts—counts from the outcome
of experiments already performed. The higher the hyperparameters are, the more
pseudo counts we have, which means our prior is “stronger.” As the total number
of experiments increases, the sum H + T also increases, which means we have less
dependence on our priors.
Initially, though, when H + T is relatively low, our prior plays a stronger role in
the estimation of θ. As we all know, a small number of flips will not give an accurate
estimate of the true θ—we’d like to see what our estimate becomes as our number
of flips approaches infinity (or some “large enough” value). In this sense, our prior
also smooths our estimation. Rather than the estimate fluctuating greatly initially,
it could stay relatively smooth if we have a decent prior.
If our prior turns out to be incorrect, eventually the observed data will over-
shadow the pseudo counts from the hyperparameters anyway, since α and β are
held constant.
A.3
Generalizing to a Multinomial Distribution
At this point, you may be able to rationalize how Dirichlet prior smoothing for
information retrieval language models or topic models works. However, our proba-
bilities are over words now, not just a binary heads or tails outcome. Before we talk
about the Dirichlet distribution, let’s figure out how to represent the probability of
observing a word from a vocabulary.
For this, we can use a categorical distribution. In a text information system, a
categorical distribution could represent a unigram language model for a single
document. Here, the total number of outcomes is k = |V |, the size of our vocabulary.
The word at index i would have a probability pi of occurring, and the sum of all
words’ probabilities would sum to one.
