13.2 Discovery of paradigmatic relations
259
N
∑
i=1
d1 = (x1, … xN)
b 2 [0, 1]
k 2 [0, +∞)
xi =
BM25(wi, d1) = 
(k + 1)c(wi, d1)
—
c(wi, d1) + k(1 – b + b * |d1|/avdl)
BM25(wi, d1)
—
BM25(wj, d1)
d2 = (y1, … yN)
yi is deﬁned similarly
IDF(wi)xi yi
Sim(d1, d2) =  
N
∑
j=1
Figure 13.6
A different similarity function based on BM25.
weight of each word by the sum of the weights of all the words. This is to ensure
all the xi’s will sum to one in this vector. This would be very similar to what we had
before, in that this vector approximates a word distribution (since the xi’s will sum
to one). For the IDF factor, the similarity function multiplies the IDF of word wi by
xiyi, which is the similarity in the ith dimension. Thus, the first problem (sublinear
scaling) is addressed in the vector representation and the second problem (lack of
IDF) is addressed in the similarity function itself.
We can also use this approach to discover syntagmatic relations. When we rep-
resent a term vector to represent a context with a term vector we would likely see
some terms have higher weights and other terms have lower weights. Depending on
how we assign weights to these terms, we might be able to use these weights to dis-
cover the words that are strongly associated with a candidate word in the context.
The idea is to use the converted representation of the context to see which terms
are scored high. If a term has high weight, then that term might be more strongly
related to the candidate word.
We have each xi defined as a normalized weight of BM25. This weight alone
reflects how frequently the wi occurs in the context. We can’t simply say a frequent
term in the context would be correlated with the candidate word because many
common words like the will occur frequently in the context. However, if we apply
IDF weighting, we can then re-weight these terms based on IDF. That means the
words that are common, like the, will get penalized. Now, the highest-weighted
terms will not be those common terms because they have lower IDFs. Instead, the
highly weighted terms would be the terms that are frequently in the context but
not frequent in the collection. Clearly, these are the words that tend to occur in the
context of the candidate word. For this reason, the highly weighted terms in this
