17.3 Mining One Topic from Text
355
θd
θB
p(θd) = 0.5
p(θB) = 0.5
text ?
the ?
the 0.9
text 0.1
d =
text the
Note that p(“text”|θd) + p(“the”|θd) = 1
Behavior 1: if p(w1|θB) > p(w2|θB), then p(w1|θd) < p(w2|θd)
→ p(“text”|θd) = 0.9  >>  p(“the”|θd) = 0.1!
If x + y = constant, then xy reaches maximum when x = y
p(d|�) = p(“text”|�)p(“the”|�)
 
 
= [0.5 * p(“text”|θd) + 0.5 * 0.1] × 
 
 
   [0.5 * p(“the”|θd) + 0.5 * 0.9]
0.5 * p(“text”|θd) + 0.5 * 0.1 = 0.5 * p(“the”|θd) + 0.5 * 0.9
Figure 17.20
Behavior of a mixture model: competition of the two component models.
If you examine the formula carefully, you might intuitively feel that we want to
set the probability of text to be somewhat larger than the, and this intuition can
indeed be supported by a mathematical fact: when the sum of two variables is a
constant, their product would reach the maximum when the two variables have the
same value.
In our case, the sum of the two terms in the product is
0.5 . p(text | θd) + 0.5 . 0.1 + 0.5 . p(the | θd) + 0.5 . 0.9 = 1.0,
so their product reaches maximum when
0.5 . p(text | θd) + 0.5 . 0.1 = 0.5 . p(the | θd) + 0.5 . 0.9.
Plugging in the constraint p(text | θd) + p(the | θd) = 1, we can easily obtain the
solution p(text | θd) = 0.9 and p(the | θd) = 0.1.
Therefore, the probability of text is indeed much larger than the probability of
the, effectively factoring out this common word. Note that this is not the case when
we have just one distribution where the has a much higher probability than text.
The effect of reducing the estimated probability of the is clearly due to the use of the
background model, which assigned very high probability to the and low probability
to text.
Looking into the process of reaching this solution, we see that the reason why
text has a higher probability than the is because its corresponding probability by
the background model p(text | θB) is smaller than that of the; had the background
model given the a smaller probability than text, our solution would give the a
