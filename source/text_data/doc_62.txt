42
Chapter 3
Text Data Understanding
Presupposition.
He has quit smoking implies that he smoked before; making
such inferences in a general way is difficult.
3.1
History and State of the Art in NLP
Research in NLP dated back to at least the 1950s when researchers were very
optimistic about having computers that understood human language, particularly
for the purpose of machine translation. Soon however, it was clear, as stated in
Bar-Hillel’s report in 1960, that fully-automatic high-quality translation could not
be accomplished without knowledge. That is, a dictionary is insufficient; instead,
we would need an encyclopedia.
Realizing that machine translation may be too ambitious, researchers tackled
less ambitious applications of NLP in the late 1960s and 1970s with some success,
though the techniques developed failed to scale up, thus only having limited ap-
plication impact. For example, people looked at speech recognition applications
where the goal is to transcribe a speech. Such a task requires only limited under-
standing of natural language, thus more realistic; for example, figuring out the
exact syntactic structure is probably not very crucial for speech recognition. Two
interesting projects that demonstrated clear ability of computer understanding of
natural language are worth mentioning. One is the Eliza project where shallow
rules are used to enable a computer to play the role of a therapist to engage a nat-
ural language dialogue with a human. The other is the block world project which
demonstrated feasibility of deep semantic understanding of natural language when
the language is limited to a toy domain with only blocks as objects.
In the 1970s–1980s, attention was paid to process real-world natural-language
text data, particularly story understanding. Many formalisms for knowledge rep-
resentation and heuristic inference rules were developed. However, the general
conclusion was that even simple stories are quite challenging to understand by
a computer, confirming the need for large-scale knowledge representation and in-
ferences under uncertainty.
After the 1980s, researchers started moving away from the traditional symbolic
(logic-based) approaches to natural language processing, which mostly had proven
to be not robust for real applications, and paying more attention to statistical
approaches, which enjoyed more success, initially in speech recognition, but later
also in virtually all other NLP tasks. In contrast to symbolic approaches, statistical
approaches tend to be more robust because they have less reliance on human-
generated rules; instead, they often take advantage of regularities and patterns in
