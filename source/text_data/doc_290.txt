270
Chapter 13
Word Association Mining
p(Xw1 = 1) =
Smoothing: Add pseudo data
so that no event has zero counts
(pretend we observed extra data)
¼ PseudoSeg_1
¼ PseudoSeg_2
¼ PseudoSeg_3
¼ PseudoSeg_4
w1
0
1
0
1
w2
0
0
1
1
Segment_1
…
Segment_N
1
0
0
1
count(w1) + 0.5
—
N + 1
p(Xw2 = 1) = count(w2) + 0.5
—
N + 1
p(Xw1 = 1, Xw2 = 1) = count(w1, w2) + 0.25
—
N + 1
Actually observed data
Figure 13.15
Smoothing in estimation of probabilities for computing mutual information.
the combinations. More specifically, you can see the 0.5 coming from the two ones
in the two pseudo-segments, because each is weighted at one quarter. If we add
them up, we get 0.5. Similar to this, 0.25 comes from one single pseudo-segment
that indicates the two words occur together. In the denominator, we add the total
number of pseudo-segments, which in this case is four pseudo-segments.
To summarize, syntagmatic relations can generally be discovered by measuring
correlations between occurrences of two words. We’ve used three concepts from
information theory: entropy, which measures the uncertainty of a random variable
X; conditional entropy, which measures the entropy of X given we know Y; and
mutual information of X and Y, which matches the entropy reduction of X due to
knowing Y, or entropy reduction of Y due to knowing X. These three concepts are
actually very useful for other applications as well. Mutual information allows us to
have values computed on different pairs of words that are comparable, allowing
us to rank these pairs and discover the strongest syntagmatic relations from a
collection of documents.
Note that there is some relation between syntagmatic relation discovery and
paradigmatic relation discovery. We already discussed the possibility of using BM25
to weight terms in the context and suggest candidates that have syntagmatic re-
lations with the target word. Here, once we use mutual information to discover
syntagmatic relations, we can also represent the context with this mutual informa-
tion as weights. This would give us another way to represent the context of a word.
And if we do the same for all the words, then we can cluster these words or compute
