6.3 Vector Space Retrieval Models
105
0
1
2
2
1
k + 1
k = 0
3
…
Term frequency weight
y = TF(w, d)
x = c(w, d)
y = (k + 1)x
—
x + k
Very large k
Figure 6.15
Illustration of BM25 TF transformation.
ensures that all terms will be counted when we aggregate the weights to compute
a score.
To summarize, we need to capture some sublinearity in the TF function. This
ensures that we represent the intuition of diminishing return from high term
counts. It also avoids a dominance by one single term over all others. The BM25
TF formula we discussed has an upper bound while being robust and effective.
If we plug this function into our TF-IDF vector space model, then we would end
up having a ranking function with a BM25 TF component. This is very close to a
state-of-the-art ranking function called BM25. We’ll see the entire BM25 formula
soon.
6.3.5
Document Length Normalization
In this section, we will discuss the issue of document length normalization. So far
in our exploration of the vector space model we considered the TF or the count of
a term in a document. We have also considered the global statistic IDF. However,
we have not considered the document length.
In Figure 6.16, we show two example documents. Document d4 is very short with
only one hundred words. Conversely, d6 has five thousand words. If you look at the
matching of these query words we see that d6 has many more matchings of the
query words; one might reason that d6 may have matched these query words in a
scattered manner. Perhaps d6’s topic is not really the same as the query’s topic. In
the beginning of d6, there is discussion of a campaign. This discussion may have
