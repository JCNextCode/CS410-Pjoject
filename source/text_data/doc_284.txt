264
Chapter 13
Word Association Mining
close to the original entropy of meat. In the case of eats, since eats is related to
meat, knowing presence or absence of eats would help us predict whether meat oc-
curs. Thus, it reduces the entropy of meat. For this reason, we expect the second
term H(Xmeat | Xeats) to have a smaller entropy, which means there is a stronger
association between these two words.
This suggests that when you use conditional entropy for mining syntagmatic
relations, the algorithm would look as follows.
1. For each word w1, enumerate all other words w2 from the corpus.
2. Compute H(Xw1 | Xw2). Sort all candidates in ascending order of the condi-
tional entropy.
3. Take the top-ranked candidate words as words that have potential syntag-
matic relations with w1.
Note that we need to use a threshold to extract the top words; this can be the
number of top candidates to take or a value cutoff for the conditional entropy.
This would allow us to mine the most strongly correlated words with a particular
word w1. But, this algorithm does not help us mine the strongest k syntagmatic
relations from the entire collection. In order to do that, we have to ensure that
these conditional entropies are comparable across different words. In this case of
discovering the syntagmatic relations for a target word like w1, we only need to
compare the conditional entropies for w1 given different words.
The conditional entropy of w1 given w2 and the conditional entropy of w1 given
w3 are comparable because they all measure how hard it is to predict the w1.
However, if we try to predict a different word other than w1, we will get a different
upper bound for the entropy calculation. This means we cannot really compare
conditional entropies across words. The next section shows how we can use mutual
information to solve this problem.
13.3.1
Mining syntagmatic relations using mutual information
The main issue with conditional entropy is that its values are not comparable across
different words, making it difficult to find the most highly correlated words in an
entire corpus. To address this problem, we can use mutual information.
In particular, the mutual information of X and Y, denoted I(X; Y), is the reduc-
tion in entropy of X obtained from knowing Y. Specifically, the question we are
interested in here is how much of a reduction in entropy of X can we obtain by
knowing Y. Mathematically, mutual information can be defined as
I(X; Y) = H(X) − H(X | Y) = H(Y) − H(Y | X).
(13.3)
