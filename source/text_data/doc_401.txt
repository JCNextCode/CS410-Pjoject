17.5 Extension of PLSA and Latent Dirichlet Allocation
381
government 0.3
response 0.2
…
city 0.2
new 0.1
orleans 0.05
…
donate 0.1
relief 0.05
help 0.02
…
Topic θk
…
…
Topic θ2
Topic θ1
PLSA → LDA
p(θk) = πd,k
p(θ2) = πd,2
p(θ1) = πd,1
Both word distributions and
topic choices are free in PLSA
LDA imposes a prior on both
p(w|θk)
W
p(w|θ2)
p(w|θ1)
θ �i = (p(w1|θi), …, p(wM|θi))
π�d = (πd,1, …, πd,k)
p(π�d) = Dirichlet(α �)
α � = (α1, …, αk), αi > 0
p(θ �i) = Dirichlet(β �)
β � = (β1, …, βM), βi > 0
Figure 17.36
Illustration of LDA as PLSA with a Dirichlet prior.
the topic coverage has k parameters, α1, . . . , αk, and the Dirichlet distribution
governing the topic word distributions has M parameters, β1, . . . , βM. Each αi can
be interpreted as the pseudo count of the corresponding topic θi according to our
prior, while each βi can be interpreted as the pseudo count of the corresponding
word wi according to our prior. With no additional knowledge, they can all be set
to uniform counts, which in effect, assumes that we do not have any preference for
any word in each word distribution and we do not have any preference for any topic
either in each document.
The likelihood function of LDA is given in Figure 17.37 where we also make
a comparison between the likelihood of PLSA and that of LDA. The comparison
allows us to see that both PLSA and LDA share the common generative model
component to define the probability of observing a word w in document d from
a mixture model involving k word distributions, θ1, . . . , θk, representing k topics
with a topic coverage distribution πd,j. Indeed, such a mixture of unigram language
models is the common component in most topic models, and is key for modeling
documents with multiple topics covered in the same document. However, the like-
lihood function for a document and the entire collection C is clearly different with
LDA adding the uncertainty of the topic coverage distribution and the uncertainty
of all the word distributions in the form of an integral.
