462
Appendix A
Bayesian Statistics
Figure A.2
How the α parameter affects the shape and sparsity of the Dirichlet distribution of
three parameters. From left to right, α = 0.1, 1, 10. (From Bishop [2006])
When used as a prior, we usually don’t have any specific information about the
individual indices in the Dirichlet. Because of this, we set them all to the same
value. So instead of writing p(θ | ⃗α), where ⃗α is (e.g.) {0.1, 0.1, . . . , 0.1}, we simply say
p(θ | α), where alpha is a scalar representing a vector of identical values. θ ∼ Dir(α)
and θ ∼ Dir(0.1) are also commonplace, as is θ ∼ Beta(α, β) or θ ∼ Beta(0.4, 0.1).
Figure A.2 shows how the choice of α characterizes the Dirichlet. The higher the
area, the more likely a point (representing a vector) will be drawn.
Let’s take a moment to understand what a point drawn from the Dirichlet means.
Look at the far right graph in Figure A.2. If the point we draw is from the peak of
the graph, we’ll get a multinomial parameter vector with a roughly equal proportion
of each of the three components. For example, θ = {0.33, 0.33, 0.34}. With α = 10,
it’s very likely that we’ll get a θ like this. In the middle picture, we’re unsure what
kind of θ we’ll draw. It is equally likely to get an even mixture, uneven mixture, or
anywhere in between. This is called a uniform prior—it can represent that we have
no explicit information about the prior distribution. Finally, the plot on the left is
a sparse prior (like a Beta where α, β < 1).
Note: a uniform prior does not mean that we get an even mixture of components;
it means it’s equally likely to get any mixture. This could be confusing since the
distribution we draw may actually be a uniform distribution.
A sparse prior is actually quite relevant in a textual application; if we have a few
dimensions with very high probability and the rest with relatively low occurrences,
this should sound just like Zipf’s law. We can use a Dirichlet prior to enforce a
sparse word distribution per topic (θ = {0.9, 0.02, 0.08}). In topic modeling, we can
use a Dirichlet distribution to force a sparse topic distribution per document. It’s
most likely that a document mainly discusses a handful of topics while the rest are
