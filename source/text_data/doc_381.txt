17.3 Mining One Topic from Text
361
For now, let’s assume we have a tentative estimate of all the parameters. How can
we infer which of the two distributions a word has been generated from? Consider
a specific word such as text. Is it more likely from θd or θB? To answer this question,
we compute the conditional probability p(θd | text). The value of p(θd | text) would
depend on two factors.
.
How often is θd (as opposed to θB) used to generate a word in general? This
probability is given by p(θd). If p(θd) is high, then we’d expect p(θd | text) to
be high.
.
If θd is indeed chosen to generate a word, how likely would we observe text?
This probability is given by p(w | θd). If p(text | θd) is high, then we’d also
expect p(θd | text) to be high.
Our intuition can be rigorously captured by using Bayes’ rule to infer p(θd | text),
where we essentially compare the product p(θd)p(text | θd) with the product p(θB)
p(text | θB) to see whether text is more likely generated from θd or from θB. This is
illustrated in Figure 17.23.
The Bayesian inference involved here is a typical one where we have some prior
about how likely each of these two distributions is used to generate any word
(i.e., p(θd) and p(θB)). These are prior because they encode our belief about which
distribution before we even observe the word text; a prior that has very high p(θd)
would encourage us to lean toward guessing θd for any word. Such a prior is then
θd
θB
p(w|θd)
p(θd) = 0.5
p(θd)p(“text”|θd)
p(θd) + p(θB) = 1
p(θB) = 0.5
p(w|θB)
text 0.04
mining 0.035
association 0.03
clustering 0.005
…
the 0.000001
Is “text” more likely
from θd or θB?
From θd (z = 0)?
Given all the parameters, infer the distribution a word is from …
p(θB)p(“text”|θB)
From θB (z = 1)?
p(z = 0|w = “text”) = 
the 0.03
a 0.02
is 0.015
we 0.01
food 0.003
…
text 0.000006
Topic choice
p(θd)p(“text”|θd)
—
p(θd)p(“text”|θd) + p(θB)p(“text”|θB)
Figure 17.23
Inference of which distribution a word is from.
