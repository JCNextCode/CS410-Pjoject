9.1 Introduction
169
ments. These are judgments of which documents should be returned for which
queries. Ideally, they have to be made by users who formulated the queries because
those are the people that know exactly what the documents (search results) would
be used for. Finally, we have to have measures to quantify how well a system’s result
matches the ideal ranked list that would be constructed based on users’ relevance
judgements.
This methodology is very useful for evaluating retrieval algorithms because the
test set can be reused many times. It also provides a fair comparison for all the
methods, since the evaluation is exactly the same for each one. That is, we have the
same criteria, same corpus, and same relevance judgements to compare the differ-
ent algorithms. This allows us to compare a new algorithm with an old algorithm
that was invented many years ago by using the same approach.
In Figure 9.1, we illustrate how the Cranfield evaluation methodology works. As
mentioned, we need a set of queries that are shown here. We have Q1, Q2, and so
on. We also need the document collection, D1, D2, . . ., and on the far right side of
the figure, we have the relevance judgments which are plus or minus annotations
on each document specifying whether it is relevant (plus) or not relevant (minus).
Essentially, these are binary judgments of documents with respect to a specific
query since there are only two levels of relevance. For example, D1 and D2 are
judged as being relevant to Q1. D3 is judged as non-relevant with respect to Q1.
D1
D2 +
D1 +
D4 –
System A
Which is better?
How to quantify?
RA  or  RB?
System B
RA
D1 +
D4 –
D3 –
D5 +
D2 +
RB
Q1  Q2  Q3
…  Q50 …
Queries
Query = Q1
Relevance
judgments
Document collection
Test Collection Evaluation
D3
D48
D2
…
Q1  D1 +
Q1  D2 +
Q1  D3 –
Q1  D4 –
Q1  D5 +
…
Q2  D1 –
Q2  D2 +
Q2  D3 +
Q2  D4 –
…
Q50  D1 –
Q50  D2 –
Q50  D3 +
Figure 9.1
Illustration of Cranfield evaluation methodology.
