15.6 Evaluation of Text Categorization
313
Algorithm 15.6
Perceptron Testing
ˆy ← w . x
return +1 if ˆy > 0, else return −1
As the algorithm shows, training of the perceptron classifier consists of contin-
uously updating the weights vector based on its performance in classifying known
examples. In the case where yi and ˆy have the same sign (classified correctly), the
weights are unchanged. In the case where yi < ˆy, the object should have been clas-
sified as −1 so weight is subtracted from each active feature index in w. In the
opposite case (yi > ˆy), weight is added to each active feature in w. By “active fea-
ture,” we mean features that are present in the current example x; only features
xij > 0 will contribute to the update in w.
Eventually, the change in w will be small after some number of iterations,
signifying that the algorithm has found the best accuracy it could. The final w vector
is saved as the model, and it can be used to classify unseen documents.
What if we need to support multiclass classification? Not all classification prob-
lems fit nicely into two categories. Fortunately, there are two common methods for
using multiple binary classifiers to create one multiclass categorization method on
k classes.
One-vs-all (OVA) trains one classifier per class (for k total classifiers). Each clas-
sifier is trained to predict +1 for its respective class and −1 for all other classes.
With this scheme, there may be ambiguities if multiple classifiers predict +1 at
test time. Because of this, linear classifiers that are able to give a confidence score
as a prediction are used. A confidence score such as +0.588 or +1.045 represents
the +1 label, but the latter is “more confident” than the former, so the class that
the algorithm predicting +1.045 would be chosen.
All-vs-all (AVA) trains k(k−1)
2
classifiers to distinguish between all pairs of k
classes. The class with the most +1predictions is chosen as the final answer. Again,
confidence-based scoring may be used to add votes into totals for each class label.
15.6
Evaluation of Text Categorization
As with information retrieval evaluation, we can use precision, recall, and F1 score
by considering true positives, false positives, true negatives, and false negatives. We
are also usually more concerned about accuracy (the number of correct predictions
divided by the number of total predictions).
