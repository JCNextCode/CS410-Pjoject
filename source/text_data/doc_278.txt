258
Chapter 13
Word Association Mining
gives us the probability that we will see an overlap on a particular word wi, where xi
gives us a probability that we will pick this particular word from d1, and yi gives us
the probability of picking this word from d2. This is how expected overlap of words
in context similarity works.
As always, we would like to assess whether this approach would work well. Ul-
timately, we have to test the approach with real data and see if it gives us really
semantically related words. Analytically, we can also analyze this formula. Initially,
itdoesmakesensebecausethisformulawillgiveahigherscoreifthereismoreover-
lap between the two contexts. However, if you analyze the formula more carefully,
then you also see there might be some potential problems.
The first problem is that it might favor matching one frequent term very well over
matching more distinct terms. That is because in the dot product, if one element
has a high value and this element is shared by both contexts, it contributes a lot
to the overall sum. It might indeed make the score higher than in another case
where the two vectors actually have much overlap in different terms. In our case,
we should intuitively prefer a case where we match more different terms in the
context, so that we have more confidence in saying that the two words indeed occur
in similar context. If you only rely on one high-scoring term, it may not be robust.
The second problem is that it treats every word equally. If we match a word like the,
it will be the same as matching a word like eats, although we know matching the
isn’t really surprising because it occurs everywhere. This is another problem of this
approach.
We can introduce some heuristics used in text retrieval that solve these prob-
lems, since problems like these also occur when we match a query with a docu-
ment. To tackle the first problem, we can use a sublinear transformation of term
frequency. That is, we don’t have to use the raw frequency count of the term to repre-
sent the context. To address this problem, we can transform it into some form that
wouldn’t emphasize the raw frequency so much. To address the second problem,
we can reward matching a rare word. A sublinear transformation of term frequency
and inverse document frequency (IDF) weighting are exactly what we’d like here;
we discussed these types of weighting schemes in Chapter 6.
In order to achieve this desired weighting, we will use BM25 weighting, which
is of course based on the BM25 retrieval function. It is able to solve the above two
problems by sublinearly transforming the count of wi in d1 and including the IDF
weighting heuristic in the similarity measure.
For this similarity scheme, we define the document vector as containing ele-
ments representing normalized BM25 TF values, as shown in Figure 13.6. The
normalization function takes a sum over all the words in order to normalize the
