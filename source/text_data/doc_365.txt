17.3 Mining One Topic from Text
345
p(w|θ)
the 0.031
a 0.018
…
text 0.04
mining 0.035
association 0.03
clustering 0.005
computer 0.0009
…
food 0.000001
…
Text mining
paper
d
Can we get rid of these 
common words?
Figure 17.12
Common words dominate the estimated unigram language model.
complicated, and numerical optimization algorithms would generally be needed
to solve the problem.
What would the topic discovered from a document look like? Let’s imagine the
document is a text mining paper. In such a case, the estimated unigram language
model (word distribution) may look like the distribution shown in Figure 17.12. On
the top, you will see the high probability words tend to be those very common words,
often function words in English. This will be followed by some content words that
really characterize the topic well like text and mining. In the end, you also see there
is a small probability of words that are not really related to the topic but might
happen to be mentioned in the document.
As a topic representation, such a distribution is not ideal because the high prob-
ability words are function words, which do not characterize the topic. Giving com-
mon words high probabilities is a direct consequence of the assumed generative
model, which uses one distribution to generate all the words in the text. How can
we improve our generative model to down-weight such common words in the es-
timated word distribution for our topic? The answer is that we can introduce a
second background word distribution into the generative model so that the com-
mon words can be generated from this background model, and thus the topic word
distribution would only need to generate the content-carrying topical words. Such a
model is called a mixture model because multiple component models are “mixed”
together to generate data. We discuss it in detail in the next section.
17.3.2
Adding a Background Language Model
In order to solve the problem of assigning highest probabilities to common words
in the estimated unigram language model based on one document, it would be
