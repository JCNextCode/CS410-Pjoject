18.1 Sentiment Classification
395
they create very unique features that machine learning programs associate as be-
ing highly correlated with a particular class label when in reality they are not. For
example, if a 7-gram phrase appears only in a positive training document, that 7-
gram would always be associated with positive sentiment. In reality, though, the
7-gram just happened to occur with the positive document and no others because
it was so rare.
We can consider n-grams of part-of-speech tags. A bigram feature could be an
adjective followed by a noun. We can mix n-grams of words and n-grams of POS tags.
For example, the word great might be followed by a noun, and this could become
a feature—a hybrid feature—that could be useful for sentiment analysis.
Next, we can have word classes. These classes can be syntactic like POS tags, or
could be semantic by representing concepts in a thesaurus or ontology like Word-
Net [Princeton University 2010]. Or, they can be recognized name entities (like
people or place), and these categories can be used to enrich the representation as
additional features. We also can learn word clusters since we’ve talked about min-
ing associations of words in Chapter 13. We can have clusters of paradigmatically
related words or syntagmatically related words, and these clusters can be features
to supplement the base word representation.
Furthermore, we can have a frequent pattern syntax which represents a frequent
word set; these are words that do not necessarily occur next to each other but often
occur in the same context. We’ll also have locations where the words may occur
more closely together, and such patterns provide more discriminative features
than words. They may generalize better than just regular n-grams because they are
frequent, meaning they are expected to occur in testing data, although they might
still face the problem of overfitting as the features become more complex. This is a
problem in general, and the same is true for parse tree-based features, e.g., frequent
subtrees. Those are even more discriminating, but they’re also more likely to cause
overfitting.
In general, pattern discovery algorithms are very useful for feature construction
because they allow us to search a large space of possible features that are more
complex than words, and natural language processing is very important to help us
derive complex features that can enrich text representations.
As we’ve mentioned in Chapter 15, feature design greatly affects categorization
accuracy and is arguably the most important part of any machine learning applica-
tion. It would be most effective if you can combine machine learning, error analysis,
and specific domain knowledge when designing features. First, we want to use do-
main knowledge, that is, a specialized understanding of the problem. With this,
we can design a basic feature space with many possible features for the machine
