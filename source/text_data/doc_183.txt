8.6 Caching
163
Hash table
Doubly linked list
K7
V4
K1
V1
K4
V7
K9
V2
K2
V12
K12
V9
Figure 8.3
An implementation of a Least-Recently Used (LRU) cache.
postings lists (keyed by term ID) will be accessed the majority of the time. But how
do we know which terms to store? Even if we knew which terms are most queried,
how do we set a cutoff for memory consumption?
The two cache designs we describe address both these issues. That is: (1) the
most-frequently used items are fast to access, and (2) we can set a maximum size
for our cache so it doesn’t get too large.
8.6.1
LRU cache
We first consider the least recently used (LRU) cache as displayed in Figure 8.3. The
LRU algorithm is as follows, assuming we want to retrieve the postings list for term
ID x.
.
First, search the cache for term ID x.
.
If it exists in the cache, return the postings list corresponding to x.
.
If it doesn’t exist in the cache, retrieve it from disk and insert it into the cache.
If this causes the cache to exceed its maximum size, remove the least-recently
used postings list.
We want searching the cache to be fast, so we use a hash table to store term IDs
as keys and postings lists as values. This enables O(1) amortized insert, find, and
delete operations. The interesting part of the LRU cache is how to determine the
access order of the objects. To do this, we link together the values as a doubly linked
list. Once an element is inserted or accessed, it is moved to the head (front) of the
