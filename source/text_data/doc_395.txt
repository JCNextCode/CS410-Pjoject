17.4 Probabilistic Latent Semantic Analysis
375
Re-estimated probability of doc d covering topic θj
Re-estimated probability of word w for topic θj
ML estimate based
on “allocated” word
counts to topic θj
Hidden variable (= topic indicator): zd,w 2 {B, 1, 2, …, k}
= 
p(n+1)(w|θj) = 
EM Algorithm for PLSA: M-Step
∑w2V c(w, d)(1 – p(zd,w = B))p(zd,w = j)
—
∑ j′∑w2Vc(w, d)(1 – p(zd,w = B))p(zd,w = j′)
∑d2C c(w, d)(1 – p(zd,w = B))p(zd,w = j)
—
∑w′2V∑d2Cc(w′, d)(1 – p(zd,w′ = B))p(zd,w′ = j)
π(n+1)
d,j
Figure 17.33
M-Step of the EM Algorithm for estimating PLSA.
we would simply collect all the split counts of words in document d that belong
to each θj, and then normalize these counts among all the k topics. Similarly, to
re-estimate p(w | θj), we would collect the split counts of a word toward θj from
all the documents in the collection, and then normalize these counts among all
the words. Note that the normalizers are very different in these two cases, which
are directly related to the constraints we have on these parameters. In the case of
re-estimation of π, the constraint is that the π values must sum to one for each
document, thus our normalizer has been chosen to ensure that the re-estimated
values of π indeed sum to one for each document. The same is true for the re-
estimation of p(w | θ), where our normalizer allows us to obtain a word distribution
for each topic.
What we observed here is actually generally true when using the EM algorithm.
That is, the distribution of the hidden variables computed in the E-step can be
used to compute the expected counts of an event, which can then be aggregated
and normalized appropriately to obtain a re-estimate of the parameters. In the
implementation of the EM algorithm, we can thus just keep the counts of various
events and then normalize them appropriately to obtain re-estimates for various
parameters.
In Figure 17.34, we show the computation of the EM algorithm for PLSA in more
detail. We first initialize all the unknown parameters randomly, including the cov-
erage distribution πd,j for each document d, and the word distribution for each
topic p(w | θj). After the initialization step, the EM algorithm would go through
