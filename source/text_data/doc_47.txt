2.1 Basics of Probability and Statistics
27
of observing this sequence is simply the product of observing each event, i.e.,
θ × (1 − θ) × θ × θ × (1 − θ) = θ3(1 − θ)2 with no adjustment for different orders
of observing three heads and two tails.
The more commonly used multinomial distribution in text analysis, which mod-
els the probability of seeing a word in a particular scenario (e.g., in a document), is
very similar to this Bernoulli distribution, just with more than two outcomes.
2.1.4
Maximum Likelihood Parameter Estimation
Now that we have a model for our coin flipping, how can we estimate its parameters
given some observed data? For example, maybe we observe the data D that we
discussed above where n = 5:
D = {h, t, h, h, t}.
Now we would like to figure out what θ is based on the observed data. Using
maximum likelihood estimation (MLE), we choose the θ that has the highest like-
lihood given our data, i.e., choose the θ such that the probability of observed data
is maximized.
To compute the MLE, we would first write down the likelihood function, i.e.,
p(D | θ), which is θ3(1 − θ)2 as we explained earlier. The problem is thus reduced
to find the θ that maximizes the function f (θ) = θ3(1 − θ)2. Equivalently, we can
attempt to maximize the log-likelihood: log f (θ) = 3 log θ + 2 log(1 − θ), since log-
arithm transformation preserves the order of values. Using knowledge of calculus,
we know that a necessary condition for a function to achieve a maximum value at a
θ value is that the derivative at the same θ value is zero. Thus, we just need to solve
the following equation:
d log f (θ)
dθ
= 3
θ −
2
1 − θ = 0,
and we easily find that the solution is θ = 3/5.
More generally, let H be the number of heads and T be the number of tails. The
MLE of the probability of heads is given by:
θMLE = arg max
θ
p(D | θ)
= arg max
θ
θH(1 − θ)T
=
H
H + T .
