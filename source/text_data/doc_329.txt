15.5 Classification Algorithms
309
pens that there are a few outliers from a different class close to our query, it will be
classified incorrectly.
There are several variations on the basic k-NN framework. For one, we can weight
the votes of the neighbors based on distance to the query in weighted k-nearest
neighbors. That is, a closer neighbor to the query would have more influence,
or a higher-weighted vote. A simple weighting scheme would be to multiply each
neighbor’s vote by 1
d , where d is the distance to the query. Thus, more distant
neighbors have less of an impact on the predicted label.
Another variant is the nearest-centroid classifier. In this algorithm, instead of
using individual documents as neighbors, we consider the centroid of each class
label (see Chapter 14 for more information on centroids and clustering). Here, if
we have n classes, we simply see which of the n is closest to the query. The centroid
of each class label may be thought of as a prototype, or ideal representation of a
document from that class. We also receive a performance benefit, since we only
need to do n similarity comparisons as opposed to a full search engine query over
all the training documents.
15.5.2
Naive Bayes
Naive Bayes is an example of a generative classifier. It creates a probability distribu-
tion of features over each class label in addition to a distribution of the class labels
themselves. This is very similar to language model topic probability calculation.
With the language model, we create a distribution for each topic. When we see a
new text object, we use our existing topics to find topic language model ˆθ that is
most likely to have generated it. Recall from Chapter 2 that
ˆθ = arg maxθ p(w1, . . . , wn | θ) = arg maxθ
n
�
i=1
p(wi | θ).
(15.1)
Algorithm 15.3
Naive Bayes Training
Calculate p(y) for each class label in the training data
Calculate p(xi | y) for each feature for each class label in the training data
Algorithm 15.4
Naive Bayes Testing
return the y ∈ Y that maximizes p(y) . �n
i=1 p(xi | y)
