Bibliographic Notes and Further Reading
187
the same documents. There are also unique documents that are only returned by
one system, so the idea of having a diverse set of result ranking methods is to ensure
the pool is broad. We can include as many possible random documents as possible.
Then, the human assessors would make complete judgements on this data set, or
pool. The remaining unjudged documents are assumed to be non-relevant and the
human annotators do not need to spend time and effort manually judging them.
If the pool is large enough, this assumption is perfectly fine. That means if your
system participates in contributing to the pool then it’s unlikely that it will be
penalized since the top-ranked documents have all been judged. However, this is
problematic for evaluating a new system that may not have contributed to the pool,
since the documents it returns may not have been judged and are assumed to be
non-relevant.
Whatwehaven’tcoveredaresomeotherevaluationstrategiessuchasA-Btesting;
this is where an evaluating system would mix the results of two methods randomly,
showing the mix of results to users. Of course, the users don’t see which result
is from which method, so the users would judge those results or click on those
documents in a search engine application. In this case, then, the system can keep
track of the clicked documents, and see if one method has contributed more to
the clicked documents. If the user tends to click on one of the results from one
method, then that method may be better. A-B testing can also be used to compare
two different retrieval interfaces.
Text retrieval evaluation is extremely important since the task is empirically
defined. If we don’t rely on users, there’s no way to tell whether one method works
better. If we have an inappropriate experiment design, we might misguide our
research or applications, drawing the wrong conclusions. The main strategy is the
Cranfield evaluation methodology for all kinds of empirical evaluation tasks (not
just for search engines). MAP and NDCG are the two main measures that you should
definitely know about since you will see them often in research papers. Finally,
retrieving up to ten documents (or some small number) is easier to interpret from
a user’s perspective since this is the number of documents they would likely see in
a real application.
Bibliographic Notes and Further Reading
Evaluation has always been an important research problem in information re-
trieval, and in empirical AI problems in general. The Cranfield evaluation meth-
odology was established in 1960s by early pioneers of information retrieval re-
searchers; important early papers on the topic can be found in Sparck Jones and
