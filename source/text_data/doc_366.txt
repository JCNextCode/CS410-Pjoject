346
Chapter 17
Topic Analysis
useful to think about why we end up having this problem. It is not hard to see that
the problem is due to two reasons. First, these common words are very frequent
in our data, thus any maximum likelihood estimator would tend to give them high
probabilities. Second, our generative model assumes all the words are generated
from one single unigram language model. The ML estimate thus has no choice
but to assign high probabilities to such common words in order to maximize the
likelihood.
Thus, in order to get rid of the common words, we must design a different gen-
erative model where the unigram language model representing the topic doesn’t
have to explain all the words in the text data. Specifically, our target topic unigram
language model should not have to generate the common words. This further sug-
gests that we must introduce another distribution to generate these common words
so that we can have a complete generative model for all the words in the document.
Since we intend for this second distribution to explain the common words, a natu-
ral choice for this distribution is the background unigram language model. We thus
have a mixture model with two component unigram language models, one being
the unknown topic that we would like to discover, and one being a background
language model that is fixed to assign high probabilities to common words.
In Figure 17.13, we see that the two distributions can be mixed together to
generate the text data, with the background model generates common words while
the topic language model to generate content-bearing words in the document.
Thus, we can expect the discovered (learned) topic unigram language model to
Topic: θd
Background (topic) θB
p(w|θd)
p(θd) = 0.5
p(θd) + p(θB) = 1
p(θB) = 0.5
p(w|θB)
…
text 0.04
mining 0.035
association 0.03
clustering 0.005
…
the 0.000001
the 0.03
a 0.02
is 0.015
we 0.01
food 0.003
…
text 0.000006
…
Text mining
paper
Topic choice
d
Figure 17.13
A two-component mixture model with a background component model.
