Exercises
297
14.5. Consider all three cluster-cluster similarity metrics discussed in this chapter.
Which of them is most likely to form “chains” of documents as opposed to a tighter
group? Why?
14.6. Implement K-means or hierarchical agglomerative clustering in META.
Make your algorithm general to any bag-of-words tokenization method by clus-
tering already-analyzed documents.
14.7. Design a heuristic to set the number of clusters (and their contents) given
a dendrogram of hierarchically clustered data. Try to make your algorithm run in
only one traversal of the tree.
14.8. Using the topic language models for semantic relatedness, rewrite the scor-
ing function using each of Dirichlet prior and Jelinek-Mercer smoothing. That is,
smooth p(w | C) in the following function:
score(w) = p(w | computer)
p(w | C)
14.9. What are the maximum and minimum values of (unnormalized) PMI?
14.10. We discussed one potential drawback to PMI and nPMI. Is there any sort
of preprocessing you can do that helps with this issue? For example, can we set a
thresholdonthetypeofwordsinthewindow, theminimumormaximumfrequency
of each word, or the implementation of the window itself?
14.11. What type of index structure could we use to efficiently store word vectors?
Assume that the distribution of values is sparse in each vector.
14.12. Design a way to cluster documents based on multiple feature types. As a
first case, consider clustering on both unigram words and unigram POS tags. As a
more advanced case, consider clustering documents via unigram words, sentence
lengths, and structural parse tree features. Hint: how would we do this in META?
14.13. In Chapter 11 we described collaborative filtering. One issue is that it does
not scale well when the number of users or items is large. Suggest a solution using
clustering that is able to provide a faster running time when many different users
need recommendations.
