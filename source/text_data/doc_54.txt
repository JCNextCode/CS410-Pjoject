34
Chapter 2
Background
Another useful concept is mutual information defined on two random variables,
I(X; Y), which is defined as the reduction of entropy of X due to knowledge about
another random variable Y, i.e.,
I(X; Y) = H(X) − H(X | Y).
(2.15)
It can be shown that mutual information can be equivalently written as
I(X; Y) = H(Y) − H(Y | X).
(2.16)
It is easy to see that I(X; Y) tends to be large if X and Y are correlated, whereas
I(X; Y) would be small if X and Y are not so related; indeed, in the extreme case
when X and Y are completely independent, there would be no reduction of entropy,
andthusH(X) = H(X | Y), andI(X; Y) = 0.However, ifX iscompletelydetermined
by Y, then H(X | Y) = 0, thus I(X; Y) = H(X). Intuitively, mutual information can
measure the correlation of two random variables. Clearly as a correlation measure
on X and Y, mutual information is symmetric.
Applications of these basic concepts, including entropy, conditional entropy,
and mutual information will be further discussed later in this book.
2.3
Machine Learning
Machine learning is a very important technique for solving many problems, and has
very broad applications. In text data management and analysis, it has also many
uses. Any in-depth treatment of this topic would clearly be beyond the scope of
this book, but here we introduce some basic concepts in machine learning that are
needed to better understand the content later in the book.
Machine learning techniques can often be classified into two types: supervised
learning and unsupervised learning. In supervised learning, a computer would
learn how to compute a function ˆy = f (x) based on a set of examples of the input
value x (called training data) and the corresponding expected output value y. It is
called “supervised” because typically the y values must be provided by humans for
each x, and thus the computer receives a form of supervision from the humans.
Once the function is learned, the computer would be able to take unseen values of
x and compute the function f (x).
When y takes a value from a finite set of values, which can be called labels, a
function f (.) can serve as a classifier in that it can be used to map an instance x
to the “right” label (or multiple correct labels when appropriate). Thus, the prob-
lem can be called a classification problem. The simplest case of the classification
problem is when we have just two labels (known as binary classification). When y
