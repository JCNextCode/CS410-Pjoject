356
Chapter 17
Topic Analysis
higher probability than text in order to ensure that the overall probability given
by the two models working together is the same for text and the. Thus, the ML
estimate tends to give a word a higher probability if the background model gives
it a smaller probability, or more generally, if one distribution has given word w1 a
higher probability than w2, then the other distribution would give word w2 a higher
probability than word w1 so that the combined probability of w1 given by the two
distributions working together would be the same as that of w2. In other words, the
two distributions tend to give high probabilities to different words as if they try to
avoid giving the high probability to the same word.
In such a two-component mixture model, we see that the two distributions
will be collaborating to maximize the probability of the observed data, but they
are also competing on the words in the sense that they would tend to “bet” high
probabilities on different words to gain advantages in this competition. In order
to make their combined probability equal (so as to maximize the product in the
likelihood function), the probability assigned by θd must be higher for a word that
has a smaller probability given by the background model θB.
The general behavior we have observed here about a mixture model is that if
one distribution assigns a higher probability to one word than another, the other
distribution would tend to do the opposite; it would discourage other distributions
to do the same. This also means that by using a background model that is fixed to
assigning high probabilities to common (stop) words, we can indeed encourage
the unknown topical word distribution to assign smaller probabilities for such
common words so as to put more probability mass on those content words that
cannot be explained well by the background model.
Let’s look at another behavior of the mixture model in Figure 17.21 by examining
the response of the estimated probabilities to the data frequencies. In Figure 17.21,
we have shown a scenario where we’ve added more words to the document, specif-
ically, more the’s to the document. What would happen to the estimated p(w | θ) if
we keep adding more and more the’s to the document?
As we add more words to the document, we would need to multiply the likelihood
function by additional terms to account for the additional occurrences. In this
case, since all the additional terms are the, we simply need to multiply by the term
representing the probability of the. This obviously changes the likelihood function,
and thus also the solution of the ML estimation. How exactly would the additional
terms accounting for multiple occurrences of the change the ML estimate? The
solution we derived earlier, p(text | θd) = 0.9, is no longer optimal. How should we
modify this solution to make it optimal for the new likelihood function?
