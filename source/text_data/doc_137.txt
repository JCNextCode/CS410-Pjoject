6.4 Probabilistic Retrieval Models
117
Document LM
Query q =
“data mining algorithms”
p(“data mining alg”|d1)
   = p(“data”|d1)
   × p(“mining”|d1)
   × p(“alg”|d1)
p(“data mining alg”|d2)
   = p(“data”|d2)
   × p(“mining”|d2)
   × p(“alg”|d2)
Document
p(w|d1)
…
text 0.2
mining 0.1
association 0.01
clustering 0.02
…
food 0.00001
…
p(w|d2)
…
food 0.25
nutrition 0.1
healthy 0.05
diet 0.02
…
d1
d2
Text mining
paper
Food nutrition
paper
Figure 6.23
Scoring a query on two documents based on their language models.
would fix our problem with zero probabilities and it’s also reasonable because we’re
now thinking of what the user is looking for in a more general way, via a unigram
language model instead of a single fixed document.
In Figure 6.23, we show two possible language models based on documents d1
and d2, and a query data mining algorithms. By making an independence assump-
tion, we could have p(q | d) as a product of the probability of each query word in
each document’s language model. We score these two documents and then rank
them based on the probabilities we calculate.
Let’s formally state our scoring process for query likelihood. A query q contains
the words
q = w1, w2, . . . , wn
such that |q| = n. The scoring or ranking function is then the probability that we
observe q given that a user is thinking of a particular document d. This is the prod-
uct of probabilities of all individual words, which is based on the independence
assumption mentioned before:
p(q | d) = p(w1 | d) × p(w2 | d) × . . . × p(wn | d).
(6.4)
