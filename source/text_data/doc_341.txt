16.3 Abstractive Text Summarization
321
si = arg maxs∈R\S
�
(1 − λ) . sim1(s, p) − λ . arg maxsj∈S sim2(s, sj)
�
.
(16.1)
The R \ S notation may be read as “R set minus S”, i.e., all the elements in R that
are not in S. The MMR formulation uses λ ∈ [0, 1] to control relevance versus re-
dundancy; the positive relevance score is discounted by the amount of redundancy
(similarity) to the already-selected sentences. Again, the two similarity metrics may
be any normalized, symmetric measures. The simplest instantiation for the sim-
ilarity metric would be cosine similarity, and this is in fact the measure used in
Carbonell and Goldstein [1998].
The algorithm may be terminated once an appropriate number of words or
sentences is in S, or if the score sim1(s, p) is below some threshold. Furthermore,
the similarity functions may be tweaked as well. Could you think of a way to include
sentence position in the similarity function? That is, if a sentence is far away
(dissimilar) from the candidate sentence, we could subtract from the similarity
score. Even better, we could interpolate the two values into a new similarity score
such as
sim(s, s′) = α . simcosine(s, s′) + (1 − α) .
�
1 −
d(s, s′)
max d(s, .)
�
,
(16.2)
where α ∈ [0, 1] controls the weight between the regular cosine similarity and the
distance measure, and d(., .) is the number of sentences between the two param-
eters. Note the “one minus” in front of the distance calculation, since a smaller
distance implies a greater similarity.
Ofcourse, λintheMMRformulaisalsoabletobeset.Infact, formulti-document
summarization, Das and Martins [2007] suggests starting out with λ = 0.3 and
then slowly increasing to λ = 0.7. The reasoning behind this is to first emphasize
novelty and then default to relevance. This should remind you of the exploration-
exploitation tradeoff discussed in Chapter 11.
16.3
Abstractive Text Summarization
An abstractive summary creates sentences that did not exist in the original docu-
ment or documents. Instead of a document vector, we will use a language model to
represent the original text. Unlike the document vector, our language model gives
us a principled way in which to generate text. Imagine we tokenized our document
with unigram words. In our language model, we would have a parameter repre-
senting the probability of each word occurring. To create our own text, we will draw
words from this probability distribution.
