14.3 Term Clustering
293
Term
Cosine similarity to “france”
spain
0.678515
belgium
0.665923
netherlands
0.652428
italy
0.633130
switzerland
0.622323
luxembourg
0.610033
portugal
0.577154
russia
0.571507
germany
0.563291
catalonia
0.534176
Figure 14.8
Using word2vec to find the most similar terms to the query “france”. (From Mikolov
et al. [2013])
vectors corresponding to the two words, w1 and w2. With such a model, we can
then try to find the vector representation for all the words that would maximize
the probability of using each word to predict all other words in a small window
of words surrounding the word. In effect, we would want the vectors representing
two semantically related words, which tend to co-occur together in a window, to be
more similar so as to generate a higher value when taking their dot product.
Google’s implementation of skip-gram, called word2vec [Mikolov et al. 2013] is
perhaps the most well-known software in this area. They showed that performing
vector addition on terms in vector space yielded interesting results. For example,
adding the vectors for Germany and capital resulted in a vector very close to the
vector Berlin. Figure 14.8 shows example output from using this tool. Although sim-
ilar results can also be obtained by using heuristic paradigmatic relation discovery
(e.g., using the methods we described in Chapter 13) and the n-gram class language
model, word embedding provides a very promising new alternative that can poten-
tially open up many interesting new applications of text mining due to its flexibility
in formulating the objective functions to be optimized and the fact that the vector
representation is systematically learned through optimizing an explicitly defined
objective function. One disadvantage of word embedding, at least in its current
form, is that the elements in the vector representation of a word are not meaning-
ful and cannot be easily interpreted intuitively. As a result, the utility of these word
vectors has so far been mostly limited to computation of word similarities, which
can also obtained by using many other methods.
