Exercises
55
the 0.03
a 0.02
is 0.015
we 0.01
. . .
computer 0.00001
. . .
the 0.032
a 0.019
is 0.014
we 0.008
computer 0.004
software 0.0001
. . .
text 0.00006
computer 400
software 150
program 104
. . .
text 3.0
. . .
the 1.1
a 0.99
is 0.9
we 0.8
Background LM: p(w|B)
General
background
English text
B
Topic LM: p(w|“computer”)
Normalized topic LM:
p(w|“computer”)/p(w|B)
All documents
containing word
“computer”
Figure 3.7
Using topic language models and a background language model to find semantically
related words.
review of statistical language models. Zhai [2008] contains a detailed discussion
of the use of statistical language models for information retrieval, some of which
will be covered in later chapters of this book. An important topic in NLP that we
have not covered much in this chapter is information extraction. A comprehensive
introduction to this topic can be found in Sarawagi [2008], and a useful survey can
be found in Jiang [2012]. For a discussion of this topic in the context of information
retrieval, see the book Moens [2006].
Exercises
3.1. In what way is NLP related to text mining?
3.2. Does poor NLP performance mean poor retrieval performance? Explain.
3.3. Given a collection of documents for a specific topic, how can we use maximum
likelihood estimation to create a topic unigram language model?
3.4. How might the size of a document collection affect the quality of a language
model?
3.5. Why might maximum likelihood estimation not be the best guess of parame-
ters for a topic language model?
