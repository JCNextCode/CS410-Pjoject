382
Chapter 17
Topic Analysis
Core assumption
in all topic models
Added by LDA
PLSA component
pd(w|{θj}, {πd,j}) =
πd,jp(w|θj)
k
∑
j=1
k
∑
j=1
log p(d|{θj}, {πd,j}) =  
Likelihood Functions for PLSA vs. LDA
PLSA
c(w, d) log [
πd,jp(w|θj)]
∑
w2V
log p(C|{θj}, {πd,j}) =  
∑ log p(d|{θj}, {πd,j})
d2C
pd(w|{θj}, {πd,j}) =
πd,jp(w|θj)
k
∑
j=1
k
∑
j=1
log p(d|α �, {θj}) = ∫  
LDA
c(w, d) log [
πd,jp(w|θj)] p(π�d|α �)dπ�d
∑
w2V
log p(C|α �, β �) = ∫  
log p(d|α �, {θj})
p(θj|β �,)dθ1…dθk
∑
d2C
k
∏
j=1
Figure 17.37
Likelihood function of PLSA and LDA.
Although the likelihood function of LDA is more complicated than PLSA, we can
still use the MLE to estimate its parameters, ⃗α and ⃗β:
(ˆ⃗α, ˆ⃗β) = arg max⃗α, ⃗β log p(C | ⃗α, ⃗β).
(17.7)
Naturally, the computation required to solve such an optimization problem is more
complicated than LDA.
It is now easy to see that LDA has only k + M parameters, far fewer than PLSA.
However, the cost is that the interesting output that we would like to generate in
topic analysis, i.e., the k word distributions {θi} characterizing all the topics in a
collection, and the topic coverage distribution {πd,j} for each document, is unfor-
tunately, no longer immediately available to us after we estimate all the parameters.
Indeed, as usually happens in Bayesian inference, to obtain values of such latent
variables in LDA, we must rely on posterior inference. That is, we must compute
