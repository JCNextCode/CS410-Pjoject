310
Chapter 15
Text Categorization
Our Naive Bayes classifier will look very similar. Essentially, we will have a feature
distribution p(xi | y) for each class label y where xi is a feature. Given an unseen
document, we will calculate the most likely class distribution that it is generated
from. That is, we wish to calculate p(y | x) for each label y ∈ Y. Let’s use our
knowledge of Bayes’ rule from Chapter 2 to rewrite this into a form we can use
programmatically given a document x.
ˆy = arg maxy∈Y p(y | x1, . . . , xn)
= arg maxy∈Y
p(y)p(x1, . . . , xn | y)
p(x1, . . . , xn)
= arg maxy∈Y p(y)p(x1, . . . , xn | y)
= arg maxy∈Y p(y)
n
�
i=1
p(xi | y)
(15.2)
Notice that we eliminate the denominator produced by Bayes’ Rule since it does not
change the arg max result. The final simplification is the independence assump-
tion that none of the features depend on one another, letting us simply multiply
all the probabilities together when finding the joint probability. It is for this reason
that Naive Bayes is called naive.
This means we need to estimate the following distributions: p(y) for all classes
and p(xi | y) for each feature in each class. This estimation is done in the exact
same way as our unigram language model estimation. That is, an easy inference
method is maximum likelihood estimation, where we count the number of times
a feature occurs in a class divided by its total number of occurrences. As discussed
in Chapter 2 this may lead to some issues with unseen words or sparse data. In this
case, we can smooth the estimated probabilities using any smoothing method we’d
like as discussed in Chapter 6. We’ve covered Dirichlet prior smoothing and Jelinek-
Mercer interpolation, among others. Finally, we need to calculate p(y), which is just
the probability of each class label. This parameter is essential when the class labels
are unbalanced; that is, we don’t want to predict a label that occurs only a few times
in the training data at the same rate that we predict the majority label.
Whereas k-NN spent most of its calculation time in testing, Naive Bayes spends
its time in training while estimating the model parameters. In testing, |Y| calcula-
tions are performed to find the most likely label. When learning the parameters,
a forward index is used so it is known which class label to attribute features to;
that is, look up the counts in each document, and update the parameter for that
