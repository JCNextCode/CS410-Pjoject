6.4 Probabilistic Retrieval Models
125
because that’s the coefficient in front of the probability of the word given by the
collection language model.
The second smoothing method we will discuss is called Dirichlet prior smooth-
ing, or Bayesian smoothing. Again, we face the problem of zero probability for words
like network. Just like Jelinek-Mercer smoothing, we’ll use the collection language
model, but in this case we’re going to combine it with the MLE esimate in a some-
what different way. The formula first can be seen as an interpolation of the MLE
probability and the collection language model as before. Instead, however, αd is
not simply a fixed λ, but a dynamic coefficient which takes μ > 0 as a parameter.
Based on Figure 6.27, we can see if we set μ to a constant, the effect is that a
long document would actually get a smaller coefficient here. Thus, a long docu-
ment would have less smoothing as we would expect, so this seems to make more
sense than fixed-coefficient smoothing. The two coefficients
|d|
|d|+μ and
μ
|d|+μ would
still sum to one, giving us a valid probability model. This smoothing can be un-
derstood as a dynamic coefficient interpolation. Another way to understand this
formula—which is even easier to remember—is to rewrite this smoothing method
in this form:
p(w | d) = c(w, d) + μ . p(w | C)
|d| + μ
.
(6.8)
Document d
Total #words = 100
p(“network”|d) =
* 0.001
p(w|d) =
=
+
μ2[0, +∞)
p(w|C)
c(w, d) + μp(w|C)
—
|d| + μ
p(“text”|d) = 10 + μ * 0.001
—
100 + μ
μ
—
100 + μ
c(w, d)
—
|d|
|d|
—
|d| + μ
μ
—
|d| + μ
Collection LM
P(w|C)
Unigram LM   p(w|θ) = ?
…
text ?
mining ?
association ?
database ?
…
query ?
network ?
10/100
5/100
3/100
3/100
1/100
0/100
text 10
mining 5
association 3
database 3
algorithm 2
…
query 1
efﬁcient 1
the 0.1
a 0.08
…
computer 0.02
database 0.01
…
text 0.001
network 0.001
mining 0.0009
…
Figure 6.27
Smoothing the query likelihood retrieval function with linear interpolation: Dirichlet
prior smoothing.
