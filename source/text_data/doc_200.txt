180
Chapter 9
Search Engine Evaluation
position, then it’s 1
2 and so on. This means we can also take an average of all the
reciprocal ranks over a set of topics, which gives us the mean reciprocal rank (MRR).
It’s a very popular measure for known item search or any problem where you have
just one relevant item.
We can see this r is quite meaningful; it indicates how much effort a user would
have to make in order to find that one relevant document. If it’s ranked on the
top it’s low effort; if it’s ranked at 100 then you actually have to (presumably) sift
through 100 documents in order to find it. Thus, r is also a meaningful measure
and the reciprocal rank will take the reciprocal of r instead of using it directly.
The usual question also applies here: Why not just simply use r? If you were
to design a ratio to measure the performance of a system where there is only one
relevant item, you might have thought about using r directly as the measure. After
all, that measures the user’s effort, right? But, think about if you take an average of
this over a large number of topics. Again, it would make a difference. For one single
topic, using r or using 1
r wouldn’t make any difference. A larger r corresponds to
a small 1
r . The difference appears when there are many topics. Just like MAP, this
sum will be dominated by large values of r. So what are those values? Those are
basically large values that indicate lower ranked results. That means the relevant
items rank very low down on the list. The average would then be dominated by
the relevant documents that are ranked in the lower portion of the list. From a
user’s perspective we care more about the highly ranked documents, so by taking
this transformation by using reciprocal rank we emphasize more on the difference
on the top. Think about the difference between rank one and rank two and the
difference between rank 100 and 1000 using each method. Is one more preferable
than the other?
In summary, we showed that the precision-recall curve can characterize the
overall accuracy of a ranked list. We emphasized that the actual utility of a ranked
list depends on how many top ranked results a user would examine; some users will
examine more than others. Average precision is a standard measure for comparing
two ranking methods; it combines precision and recall while being sensitive to the
rank of every relevant document. We concluded this section with three methods to
summarize multiple average precision values: MAP, gMAP, and MRR.
9.4
Evaluation with Multi-level Judgements
In this section, we will explain how to evaluate text retrieval systems when there
are multiple levels of relevance judgments. So far we discussed about binary
judgements—that means a documents is judged as being relevant or non-relevant.
