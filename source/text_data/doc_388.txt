368
Chapter 17
Topic Analysis
as a result, our M-step has an analytical solution, thus we were able to bypass the
explicit computation of this auxiliary function and go directly to find a re-estimate
of the parameters. Thus in the E-step, we only computed a key component in the
auxiliary function, which is the probability that a word has been generated from
each of the two distributions, and our M-step directly gives us an analytical solution
to the problem of optimizing the auxiliary function, and the solution directly uses
the values obtained from the E-step.
The EM algorithm has many applications. For example, in general, parameter es-
timation of all mixture models can be done by using the EM algorithm. The hidden
variables introduced in a mixture model often indicate which component model
has been used to generate a data point. Thus, once we know the values of these
hidden variables, we would be able to partition data and identify the data points
that are likely generated from any particular distribution, thus facilitating estima-
tion of component model parameters. In general, when we apply the EM algorithm,
we would augment our data with supplementary unobserved hidden variables to
simplify the estimation problem. The EM algorithm would then work as follows.
First, it would randomly initialize all the parameters to be estimated. Second, in
the E-step, it would attempt to infer the values of the hidden variables based on the
current generation of parameters, and obtain a probability distribution of hidden
variables over all possible values of these hidden variables. Intuitively, this is to take
a good guess of the values of the hidden variables. Third, in the M-step, it would
use the inferred hidden variable values to compute an improved estimate of the
parameter values. This process is repeated until convergence to a local maximum
of the likelihood function. Note that although the likelihood function is guaran-
teed to converge to a local maximum, there is no guarantee that the parameters to
be estimated always have a stable convergence to a particular set of values. That is,
the parameters may oscillate even though the likelihood is increasing. Only if some
conditions are satisfied would the parameters be guaranteed to converge (see Wu
1983).
17.4
Probabilistic Latent Semantic Analysis
In this section, we introduce probabilistic latent semantic analysis (PLSA), the most
basic topic model, with many applications. In short, PLSA is simply a generalization
of the two-component mixture model that we discussed earlier in this chapter to
discover more than one topic from text data. Thus, if you have understood the two-
component mixture model, it would be straightforward to understand how PLSA
works.
