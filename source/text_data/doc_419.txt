18.2 Ordinal Regression
399
Classiﬁer 1
p(r ≥ j|X) = eαj+∑M
i=1xiβi
—
eαj+∑M
i=1xiβi + 1
How many parameters are there in total? 
 → Share training data
Key idea: 8i = 1, …, M, 8j = 3, …, k, βji = βj–1i  
 → Reduce number of parameters
M + k – 1
= log 
log 
= αj + ∑M
i=1xiβi  βi 2 < 
p(Yj = 1|X)
—
p(Yj = 0|X)
p(r ≥ j|X)
—
1 – p(r ≥ j|X)
Rating
Classiﬁer 2
Classiﬁer k – 1
k – 1
k – 2
…
2
1
k
Figure 18.7
The idea of ordinal logistic regression.
precisely the idea of ordinal logistic regression, which is an improvement over the
k − 1 independent logistic regression classifiers, as shown in Figure 18.7.
The improvement is to tie the β parameters together; that means we are going
to assume the β values are the same for all the k − 1 classifiers. This encodes our
intuition that positive words (in general) would make a higher rating more likely.
In fact, this would allow us to have two benefits. One is to reduce the number of
parameters significantly. The other is to allow us to share the training data amongst
all classifiers since the parameters are the same. In effect, we have more data to help
us choose good β values.
The resulting formula would look very similar to what we’ve seen before, only
now the β parameter has just one index that corresponds to a single feature; it
no longer has the other indices that correspond to rating levels. However, each
classifier still has a distinct predicted rating value. Of course, this value is needed
to predict the different rating levels. So αj is different since it depends on j, but the
rest of the parameters (the βi’s) are the same. We now have M + k − 1 parameters.
It turns out that with this idea of tying all the parameters, we end up having a
similar way to make decisions, as shown in Figure 18.8.
More specifically, the criteria whether the predictor probabilities are at least 0.5
or above is equivalent to whether the score of the object is larger than or equal to αk.
The scoring function is just taking a linear combination of all the features with the β
values. This means now we can simply make a rating decision by looking at the value
of this scoring function and seeing which bracket it falls into. In this approach,
we’re going to score the object by using the features and trained parameter values.
