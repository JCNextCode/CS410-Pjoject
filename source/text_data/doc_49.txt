2.1 Basics of Probability and Statistics
29
p(D) =
�
θ′ p(θ′)p(D | θ′)dθ′
(2.9)
which means the probability for a particular θ is
p(θ | D) =
p(D | θ)p(θ)
�
θ′ p(θ′)p(D | θ′)dθ′ .
(2.10)
We have special names for these quantities:
.
p(θ | D): the posterior probability of θ
.
p(θ): the prior probability of θ
.
p(D | θ): the likelihood of D
.
�
θ′ p(θ′)p(D | θ′)dθ′: the marginal likelihood of D
The last one is called the marginal likelihood because the integration “marginal-
izes out” (removes) the parameter θ from the equation. Since the likelihood of the
data remains constant, observing the constraint that p(θ | D) must sum to one over
all possible values of θ, we usually just say
p(θ | D) ∝ p(θ)p(D | θ).
That is, the posterior is proportional to the prior times the likelihood.
The posterior distribution of the parameter θ fully characterizes the uncertainty
of the parameter value and can be used to infer any quantity that depends on the
parameter value, including computing a point estimate of the parameter (i.e., a
single value of the parameter). There are multiple ways to compute a point estimate
based on a posterior distribution. One possibility is to compute the mean of the
posterior distribution, which is given by the weighted sum of probabilities and the
parameter values. For a discrete distribution,
E[X] =
�
x
xp(x)
(2.11)
while in a continuous distribution,
E[X] =
�
x
xf (x)dx
(2.12)
Sometimes, we are interested in using the mode of the posterior distribution
as our estimate of the parameter, which is called Maximum a Posteriori (MAP)
estimate, given by:
θMAP = arg max
θ
p(θ | D) = arg max
θ
p(D | θ)p(θ).
(2.13)
