15.5 Classification Algorithms
311
document’s true class label. An inverted index is not necessary for this usage. Mem-
ory aside from the forward index is required to store the parameters, which can be
represented as O(|Y| + |V | . |Y|) floating point numbers.
Due to its simplicity and strong independence assumptions, Naive Bayes is often
outperformed by more sophisticated classification methods, many of which are
based on the linear classifiers discussed in the next section.
15.5.3
Linear Classifiers
Linear classifiers are inherently binary classifiers. Consider the following linear
classifier:
f (x) =
� +1
if w . x > 0
−1
otherwise
(15.3)
It takes the dot product between the unseen document vector and the weights
vector w (where |w| = |x| = |V |). Training a linear classifier is learning (setting) the
values in the weights vector w such that dotting it with a document vector produces
a value greater than 0 for a positive instance and less than zero for a negative
instance. There are many such algorithms, including the state-of-the-art support
vector machines (SVM) classifier [Campbell and Ying 2011].
We call this group of learning algorithms linear classifiers because their decision
isbasedonalinearcombinationoffeatureweights(w andx).Figure15.4showshow
the dot product combination creates a decision boundary between two label groups
plotted in a simple two-dimensional example. Two possible decision boundaries
Figure 15.4
Two decision boundaries created by linear classifiers on two classes in two dimensions.
