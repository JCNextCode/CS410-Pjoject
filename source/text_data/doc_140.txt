120
Chapter 6
Retrieval Models
log p(q|d) = ∑
w2V
c(w, q)log p(w|d)
= 
∑
w2V,c(w,d)>0
c(w, q)log pSeen(w|d)   +
= 
+   |q|log αd   + 
∑
w2V,c(w,d)>0
c(w, q)log
∑
w2V
c(w, q)log p(w|C)
pSeen(w|d)
—
αd p(w|C)
∑
w2V,c(w,d)=0
c(w, q)log αd p(w|C)
∑
w2V,c(w,d)>0
c(w, q)log αd p(w|C)
∑
w2V
c(w, q)log αd p(w|C)      –
Query words not matched in d
Query words matched in d
Query words matched in d
All query words
Figure 6.24
Substituting smoothed probabilities into the query likelihood retrieval formula.
to unseen words. Regardless of whether the word w is seen in the document or not,
all these probabilities must sum to one, so αd is constrained.
Now that we have this smoothing formula, we can plug it into our query likeli-
hood ranking function, illustrated in Figure 6.24. In this formula, we have a sum
over all the query words, written in the form of a sum over the corpus vocabulary.
Although we sum over words in the vocabulary, in effect we are just taking a sum of
query words since each word is weighted by its frequency in the query. Such a way to
write this sum is convenient in some transformations. In our smoothing method,
we’re assuming the words that are not observed in the document have a somewhat
different form of probability. Using this form we can decompose this sum into two
parts: one over all the query words that are matched in the document and the other
over all the words that are not matched. These unmatched words have a different
form of probability because of our assumption about smoothing.
We can then rewrite the second sum (of query words not matched in d) as a
difference between the scores of all words in the vocabulary minus all the query
words matched in d. This is actually quite useful, since part of the sum over all w ∈ V
can now be written as |q| log αd. Additionally, the sum of query words matched in d
