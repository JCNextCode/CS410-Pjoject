364
Chapter 17
Topic Analysis
token, but we don’t really observe these z values. This is why we referred to such a
variable as a hidden variable.
A main idea of EM is to leverage such hidden variables to simplify the computa-
tion of the ML estimate since knowing the values of these hidden variables makes
the ML estimate trivial to compute; we can pool together all the words whose z val-
ues are 0 and normalize their counts. Knowing z values can potentially help simplify
the task of computing the ML estimate, and EM exploits this fact by alternating the
E-step and M-step in each iteration so as to improve the parameter estimate in a
hill-climbing manner.
Specifically, the E-step is to infer the value of z for all the words, while the M-step
is to use the inferred z values to split word counts between the two distributions,
and use the allocated counts for θd to improve its estimation, leading to a new
generation of improved parameter values, which can then be used to perform a
new iteration of E-step and M-step to further improve the parameter estimation.
In the M-step, we adjust the count c(w, d) based on p(z = 0 | w) (i.e., probability
that the word w is indeed from θd) so as to obtain a discounted count c(w, d)p(z =
0 | w) which can be interpreted as the expected count of the event that word w is
generatedfromθd.Similarly, θB hasitsownshareofthecount, whichisc(w, d)p(z =
1 | w) = c(w, d)[1 − p(z = 0 | w)], and we have
c(w, d)p(z = 0 | w) + c(w, d)p(z = 1 | w) = c(w, d),
(17.5)
showing that all the counts of word w have been split between the two distributions.
Thus, the M-step is simply to normalize these discounted counts for all the words
toobtainaprobabilitydistributionoverallthewords, whichcanthenberegardedas
our improved estimate of p(w | θd). Note that in the M-step, if p(z = 0 | w) = 1for all
words, we would simply compute the simple single unigram language model based
on all the observed words (which makes sense since the E-step would have told us
that there is no chance that any word has been generated from the background).
In Figure 17.25, we further illustrate in detail what happens in each iteration of
the EM algorithm. First, note that we used superscripts in the formulas of the E-
step and M-step to indicate the generation of parameters. Thus, the M-step is seen
to use the n-th generation of parameters together with the newly inferred z values
to obtain a new (n + 1)th generation of parameters (i.e., pn+1(w | θd)). Second, we
assume the two component models (θd and θB) have equal probabilities; we also
assume that the background model word distribution is known (fixed as shown in
the third column of the table).
The computation of EM starts with preparation of relevant word counts. Here
we assume that we have just four words, and their counts in the observed text data
