380
Chapter 17
Topic Analysis
coverage distribution π can be added in a similar way to the updating formula for
πd,j to force the updated parameter value to give some topics higher probabilities
by reducing the probabilities of others. In the extreme, it is also possible to achieve
the effect of setting the probability of a topic to zero by using an infinitely strong
prior that gives such a topic a zero probability.
PLSA is a generative model for modeling the words in a given document, but it
is not a generative model for documents since it cannot give a probability of a new
unseen document; it cannot give a distribution over all the possible documents.
However, we sometimes would like to have a generative model for documents. For
example, if we can estimate such a model for documents in each topic category,
then we would be able to use the model for text categorization by comparing the
probability of observing a document from the generative model of each category
and assigning the document to the category whose generative model gives the high-
est probability to the document. The difficulty in giving a new unseen document
a probability using PLSA is that the topic coverage parameter in PLSA is tied to an
observed document, and we do not have available in the model the coverage of top-
ics in a new unseen document, which is needed in order to generate words in a
new document. Although it is possible to use a heuristic approach to estimate the
topic coverage in an unseen document, a more principled way to solve the prob-
lem is to add priors on the parameters of PLSA and make a Bayesian version of the
model. This has led to the development of the Latent Dirichlet Allocation (LDA)
model.
Specifically, in LDA, the topic coverage distribution (a multinomial distribution)
for each document is assumed to be drawn from a prior Dirichlet distribution,
which defines a distribution over the entire space of the parameters of a multi-
nomial distribution, i.e., a vector of probabilities of topics. Similarly, all the word
distributions representing the latent topics in a collection of text are also assumed
to be drawn from another Dirichlet distribution. In PLSA, both the topic coverage
distribution and the word distributions are assumed to be (unknown) parameters
in the model. In LDA, they are no longer parameters of the model since they are
assumed to be drawn from the corresponding Dirichlet (prior) distributions.
Thus, LDA only has parameters to characterize these two kinds of Dirichlet
distributions. Once these parameters are fixed, the behavior of these two Dirichlet
distributions would be fixed, and thus the behavior of the entire generative model
would also be fixed. Once we have sampled all the word distributions for the whole
collection (which shares these topics), and the topic coverage distribution for a
document, the rest of the process of generating words in the document is exactly
the same as in PLSA. The generalization of PLSA to LDA by imposing Dirichlet priors
is illustrated in Figure 17.36, where we see that the Dirichlet distribution governing
