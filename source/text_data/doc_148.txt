128
Chapter 6
Retrieval Models
which adds pseudo counts proportional to the probability of the current word in
the background collection. In most cases we can see, by using these smoothing
methods, we will be able to reach a retrieval function where the assumptions are
clearly articulated, making them less heuristic than some of the vector space mod-
els. Even though we didn’t explicitly set out to define the popular VS heuristics, in
the end we naturally arrived at TF-IDF weighting and document length normaliza-
tion, perhaps justifying their inclusion in the VS models. Each of these functions
also has a smoothing parameter (λ or μ) with an intuitive meaning. Still, we need to
set these smoothing parameters or estimate them in some way. Overall, this shows
that by using a probabilistic model, we follow very different strategies than the vec-
tor space model. Yet in the end, we end up with retrieval functions that look very
similar to the vector space model. Some advantages here are having assumptions
clearly stated and a final form dictated by a probabilistic model.
This section also concludes our discussion of the query likelihood probabilistic
retrieval models. Let’s recall what assumptions we have made in order to derive the
functions that we have seen the following.
1. The relevance can be modeled by the query likelihood, i.e., p(R | d, q) ≈
p(q | d).
2. Query words are generated independently, allowing us to decompose the
probability of the whole query into a product of probabilities of observed
words in the query.
3. If a word is not seen in the document, its probability is proportional to its
probability in the collection (smoothing with the background collection).
4. Finally, we made one of two assumptions about the smoothing, using either
Jelinek-Mercer smoothing or Dirichlet prior smoothing.
If we make these four assumptions, then we have no choice but to take the
form of the retrieval function that we have seen earlier. Fortunately, the function
has a nice property in that it implements TF-IDF weighting and document length
normalization. In practice, these functions also work very well. In that sense, these
functions are less heuristic compared with the vector space model.
Bibliographic Notes and Further Reading
A brief review of many different kinds of retrieval models can be found in Chapter
2 Zhai [2008]. The vector space model with pivoted length normalization was pro-
posed and discussed in detail in Singhal et al. [1996]. The query likelihood retrieval
model was initially proposed in Ponte and Croft [1998]. A useful reference for the
