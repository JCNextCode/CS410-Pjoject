288
Chapter 14
Text Clustering
Depending on our application, we can define the context as the aforementioned
window of size n, a sentence, a document, and so on. Changing the context modifies
the interpretation of PMI—for example, if we only considered a context to be of
size n = 1, we will get significantly different results than if we set the context to
be an entire document from the corpus. In order to have comparable PMI scores,
we also need to ensure that our PMI measure is symmetric; this again depends
on our definition of context. If we define context to be “wj follows wi”, then
pmi(wi, wj) ̸= pmi(wj , wi), which is required to cluster terms.
It is possible to normalize the PMI score in the range [0, 1]:
npmi(wi, wj) =
pmi(wi, wj)
− log p(wi, wj) ,
(14.8)
making comparisons between different word pairs possible. However, this normal-
ization doesn’t fix a major issue in the PMI formula itself. Imagine that we have a
rare word that always occurs in the context of another (perhaps very common) word.
It would seem that this word pair is very highly related, but in fact our data is just
too sparse to model the connection appropriately. This problem can be alleviated
by using the mutual information measure introduced in Chapter 13 which consid-
ers not just the case when the rare word is observed, but also the case when it is not
observed. Indeed, since mutual information is bounded by the entropy of one of
the two variables, and a rare word has very low entropy, it generally wouldn’t have
a high mutual information with any other word.
Despite their drawbacks, however, PMI and nPMI are often used in practice and
are also useful building blocks for more advanced methods as well as allowing us
to understand the basic idea behind information capture in word co-occurrence.
We thus included a discussion in this book.
Below we will briefly introduce two advanced methods for term clustering. The
windowing technique employed here is critical in both of the following advanced
methods.
14.3.3
Advanced Methods
In this section, we introduce two advanced methods for term clustering.
14.3.3.1
N-gram Class Language Models
Brown clustering [Brown et al. 1992] is a model-based term clustering algorithm
that constructs term clusters (called word classes) to maximize the likelihood of
an n-gram class language model. However, since the optimization problem is in-
tractable to solve computationally, the actual process of constructing term clusters
is actually similar to hierarchical agglomerative clustering where single words are
