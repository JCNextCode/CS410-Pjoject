312
Chapter 15
Text Categorization
Algorithm 15.5
Perceptron Training
w ← {0, . . . , 0}
for iteration t ∈ T do
for each training element i do
ˆy = w . xi
wj = wj + α(yi − ˆy) . xij , ∀j ∈ [0, |V |]
end for
break if change in w is small
end for
are shown; the almost vertical line barely separates the two classes while the other
line has a wide margin between the two classes. The SVM algorithm mentioned
in the previous paragraph attempts to maximize the margin between the decision
boundary and the two classes, thus leaving more “room” for new examples to be
classified correctly, as they will fall on the side of the decision boundary close to
the examples of the same class.
Of course, it’s possible that not all data points are linearly separable, so the de-
cision boundary will be created such that it splits the two classes as accurately as
possible. Naive Bayes can also be shown to be a linear classifier. This is in contrast
to k-NN—since it only considers a local subspace in which the query is plotted, it
ignores the rest of the corpus and no lines are drawn. Some more advanced meth-
ods such as the kernel trick may change linear classifiers into nonlinear classifiers,
but we refer the reader to a text more focused on machine learning for the de-
tails [Bishop 2006].
In this book, we choose to examine the relatively simple perceptron classifier, on
whichmanyotherlinearclassifiersarebased.Weneedtospecifyseveralparameters
which are used in the training algorithm. Let T represent the maximum number
of iterations to run training for. Let α > 0 be the learning rate. The learning rate
controlsbyhowmuchweadjusttheweightsateachstep.Wemayterminatetraining
early if the change in w is small; this is usually measured by comparing the norm
of the current iteration’s weights to the norm of the previous iteration’s weights.
There are many discussions about the choice of learning rate, convergence
criteria and more, but we do not discuss these in this book. Instead, we hope to
familiarize the reader with the general spirit of the algorithm, and again refer the
reader to Bishop [2006] for many more details on algorithm implementation and
theoretical properties.
