50
Chapter 3
Text Data Understanding
analysis generally requires syntactical structure representation. We can also gener-
ate structure-based features that might help us classify the text objects into differ-
ent categories by looking at their different syntactic structures. If you want to clas-
sify articles into different categories corresponding to different authors, then you
generally need to look at syntactic structures. When we add entities and relations,
then we can enable other techniques such as knowledge graphs or information net-
works. Using these more advanced feature representations allows applications that
deal with entities.
Finally, when we add logical predicates, we can integrate analysis of scattered
knowledge. For example, we can add an ontology on top of extracted information
from text to make inferences. A good example of an application enabled by this
level of representation is a knowledge assistant for biologists. This system is able
tomanagealltherelevantknowledgefromliteratureaboutaresearchproblemsuch
as understanding gene functions. The computer can make inferences about some
of the hypotheses that a biologist might be interested in. For example, it could
determine whether a gene has a certain function by reading literature to extract
relevant facts. It could use a logic system to track answers to researchersâ€™ questions
about what genes are related to what functions. In order to support this level of
application, we need to go as far as logical representation.
This book covers techniques mainly focused on word-based representation.
These techniques are general and robust and widely used in various applications.
In fact, in virtually all text mining applications, you need this level of represen-
tation. Still, other levels can be combined in order to support more linguistically
sophisticated applications as needed.
3.4
Statistical Language Models
A statistical language model (or just language model for short) is a probability
distribution over word sequences. It thus gives any sequence of words a potentially
different probability. For example, a language model may give the following three-
word sequences different probabilities:
p(Today is Wednesday) = 0.001
p(Today Wednesday is) = 0.000000001
p(The equation has a solution) = 0.000001
Clearly, a language model can be context-dependent. In the language model
shown above, the sequence The equation has a solution has a smaller probability
than Today is Wednesday. This may be a reasonable language model for describ-
