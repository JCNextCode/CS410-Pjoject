Bibliographic Notes and Further Reading
385
ple, we can aggregate the documents in a particular time period to assess the
coverage of a particular topic in the time period. This would allow us to gener-
ate a temporal trend of topics. We can also aggregate topics covered in documents
associated with a particular author to reveal the expertise areas of the author. Fur-
thermore, we can also cluster terms and cluster documents. In fact, each topic
word distribution can be regarded as a cluster (for example, the cluster can be eas-
ily obtained by selecting the top N words with the highest probabilities). So we
can generate term clusters easily based on the output from PLSA. Documents can
also be clustered in the same way: we can assign a document to the topic cluster
that’s covered most in the document. Recall that πd,j indicates to what extent each
topic θj is covered in document d. We can thus assign the document to the top-
ical cluster that has the highest πd,j. Another use of the results from PLSA is to
treat the inferred topic coverage distribution in a document as an alternative way
of representing the document in a low-dimensional semantic space where each
dimension corresponds to a topic. Such a representation can supplement the bag-
of-words representation to enhance inexact matching of words in the same topic,
which can generally be beneficial (e.g., for information retrieval, text clustering,
and text categorization).
Finally, a variant of PLSA called latent Dirichlet allocation (LDA) extends PLSA
by adding priors to the document-topic distributions and topic-word distributions.
These priors can force a small number of topics to dominate in each document,
which makes sense because usually a document is only about one or two topics
as opposed to a true mixture of all k topics. Secondly, adding these priors can
give us sparse word distributions in each topic as well, which mimics the Zipfian
distribution of words we’ve discussed previously. Finally, LDA is a generative model,
which can be used to simulate (generate) values of parameters in the model as well
as apply the model to a new, unseen document [Blei et al. 2003].
Bibliographic Notes and Further Reading
We’ve mentioned the original PLSA paper [Hofmann 1999] and its successor LDA
[Blei et al. 2003]. Asuncion et al. [2009] compares various inference methods for
topic models and concludes that they are all very similar. For evaluation, we’ve
referenced Chang et al. [2009] in this chapter, and it showed that convenient math-
ematical measures such as log-likelihood are not correlated with human measures.
For books, Koller and Friedman [2009] is a large and detailed introduction to prob-
abilistic graphical models. Bishop [2006] covers graphical models, mixture models,
EM, and inference in the larger scope of machine learning. Steyvers and Griffiths
