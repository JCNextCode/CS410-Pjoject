13.3 Discovery of Syntagmatic Relations
263
H(Xmeat | Xeats) =
�
u∈{0,1}
p(Xeats = u)H(Xmeat | Xeats = u)
=
�
u∈{0,1}
p(Xeats = u)
. �
v∈{0,1}
�
−p(Xmeat = v | Xeats = u) log2 p(Xmeat = v | Xeats = u)
�
This formula considers both scenarios of the value of eats and captures the condi-
tional entropy regardless of whether eats is equal to 1 or 0 (present or absent). We
define the conditional entropy of meat given eats as the following expected entropy
of meat for both values of eat:
H(Xmeat | Xeats) =
�
u∈{0,1}
p(Xeats = u)H(Xmeat | Xeats = u).
(13.1)
In general, for any discrete random variables X and Y, we have the conditional
entropy is no larger than the entropy of the variable X; that is,
H(X) ≥ H(X | Y).
(13.2)
This is an upper bound for the conditional entropy. The inequality states that
we can only reduce uncertainty by adding more information, which makes sense.
As we know more information, it should always help us make the prediction and
can’t hurt the prediction in any case.
This conditional entropy gives us one way to measure the association of two
words because it tells us to what extent we can predict one word given that we know
the presence or absence of another word.
Before we look at the intuition of conditional entropy in capturing syntagmatic
relations, it’s useful to think of a very special case of the conditional entropy of a
word given itself: H(Xmeat | Xmeat). This means we know where meat occurs in the
sentence, and we hope to predict whether the meat occurs in the sentence. This is
zero because once we know whether the word occurs in the segment, we’ll already
know the answer of the prediction! That also happens to be when this conditional
entropy reaches the minimum.
Let’s look at some other cases. One is knowing the and trying to predict meat.
Another is the case of knowing eats and trying to predict meat. We can ask the ques-
tion: which is smaller, H(Xmeat | Xthe) or H(Xmeat | Xeats)? We know that smaller
entropy means it is easier to predict.
In the first case, the doesn’t really tell us much about meat; knowing the oc-
currence of the doesn’t really help us reduce entropy that much, so it stays fairly
