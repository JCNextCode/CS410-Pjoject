54
Chapter 3
Text Data Understanding
the 0.03
a 0.02
is 0.015
we 0.01
. . .
food 0.003
computer 0.00001
. . .
text 0.000006
. . .
the 0.032
a 0.019
is 0.014
we 0.011
. . .
computer 0.004
software 0.0001
. . .
text 0.00006
. . .
the 0.031
. . .
text 0.04
mining 0.035
association 0.03
clustering 0.005
computer 0.0009
. . .
food 0.000001
. . .
Background LM: p(w|B)
General
background
English text
B
Document LM: p(w|D)
Collection LM: p(w|C)
Computer
science
papers
Text mining
paper
C
D
Figure 3.6
Three different language models representing three different topics.
the purpose well. So we can use the background language model to normalize the
model p(w | computer) and obtain a probability ratio for each word. Words with
high ratio values can then be assumed to be semantically associated with computer
since they tend to occur frequently in its context, but not frequently in general. This
is illustrated in Figure 3.7.
More applications of language models in text information systems will be fur-
ther discussed as their specific applications appear in later chapters. For example,
we can represent both documents and queries as being generated from some lan-
guage model. Given this background however, the reader should have sufficient
information to understand the future chapters dealing with this powerful statisti-
cal tool.
Bibliographic Notes and Further Reading
There are many textbooks on NLP, including, Speech and Language Processing
[Jurafsky and Martin 2009], Foundations of Statistical NLP [Manning and SchÂ¨utze
1999], and Natural Language Understanding [Allen 1995]. An in-depth coverage of
statistical language models can be found in the book Statistical Methods for Speech
Recognition [Jelinek 1997]. Rosenfeld [2000] provides a concise yet comprehensive
