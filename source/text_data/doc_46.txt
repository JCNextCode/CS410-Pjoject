26
Chapter 2
Background
Specifically, we may view random variable X as denoting our hypothesis, and Y
as denoting the observed evidence. p(X) can thus be interpreted as our prior belief
about which hypothesis is true; it is “prior” because it is our belief before we have
any knowledge about evidence Y. In contrast, p(X | Y) encodes our posterior belief
about the hypothesis since it is our belief after knowing evidence Y. Bayes’ rule is
seen to connect the prior belief and posterior belief, and provide a way to update
the prior belief p(X) based on the likelihood of the observed evidence Y and obtain
the posterior belief p(X | Y). It is clear that if X and Y are independent, then no
updating will happen as in this case, p(X | Y) = p(X).
2.1.3
Coin Flips and the Binomial Distribution
In most discussions on probability, a good example to investigate is flipping a
coin. For example, we may be interested in modeling the presence or absence of a
particular word in a text document, which can be easily mapped to a coin flipping
problem. There are two possible outcomes in coin flipping: heads or tails. The
probability of heads is denoted as θ, which means the probability of tails is 1 − θ.
To model the probability of success (in our case, “heads”), we can use the
Bernoulli distribution. The Bernoulli distribution gives the probability of success
for a single event—flipping the coin once. If we want to model n throws and find the
probability of k successes, we instead use the binomial distribution. The binomial
distribution is a discrete distribution since k is an integer. We can write it as
p(k heads) =
�n
k
�
θk(1 − θ)n−k.
(2.6)
We can also write it as follows:
p(k heads) =
n!
k!(n − k)!θk(1 − θ)n−k.
(2.7)
But why is it this formula? Well, let’s break it apart. If we have n total binary
trials, and want to see k heads, that means we must have flipped k heads and n − k
tails. The probability of observing each of the k heads is θ, while the probability
of observing each of the remaining n − k tails is 1 − θ. Since we assume all these
flips are independent, we simply multiply all the outcomes together. Since we don’t
care about the order of the outcomes, we additionally multiply by the number of
possible ways to choose k items from a set of n items.
What if we do care about the order of the outcomes? For example, what is the
probability of observing the particular sequence of outcomes (h, t, h, h, t) where h
and t denote heads and tails, respectively? Well, it is easy to see that the probability
