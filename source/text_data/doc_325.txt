15.4 Features for Text Categorization
305
Clearly, a bigram words representation would most likely give better perfor-
mance since we can capture not good and not bad as well as was good and was bad.
As a counterexample, using only bigram words leads us to miss out on rarer infor-
mative single words such as overhyped. This term is now captured in bigrams such
as overhyped (period) and very overhyped. If we see the same rarer informative word
in a different context—such as was overhyped—this is now an out-of-vocabulary
term and can’t be used in determining the sentence polarity. Due to this phe-
nomenon, it is very common to combine multiple feature sets together. In this
case, we can tokenize documents with both unigram and bigram words.
A well-known strategy discussed in Stamatatos [2009] shows that low-level lexical
features combined with high-level syntactic features give the best performance in a
classifier. These two types of features are more orthogonal, thus capturing different
perspectives of the text to enrich the feature space. Having many different types of
features allows the classifier a wide range of space on which to create a decision
boundary between different class labels.
An example of very high-level features can be found in Massung et al. [2013]. Con-
sider the grammatical parse tree discussed in Chapter 4 reproduced in Figure 15.2.
many
theoretical
ideas
JJ
JJ
NNS
have
VBP
They
PRP
NP
NP
VP
S
S
NP
NP
PRP
VP
NP
VBP
NP
JJ
JJ
NNS
VP
x
x
x
x
x
x
x
x
x
x
x
x
x
x x
x
x x
x
x
x
x
S
x
x
x
x
x
x
VP
NP
x
x
x
x
x x
x
x x
x
NP
x
x
Figure 15.2
A grammatical parse tree and different feature representations derived from it. For
each feature type, each dimension in a feature vector would correspond to a weight of
a particular parse tree structure.
