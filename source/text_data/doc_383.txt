17.3 Mining One Topic from Text
363
With the E-step and M-step as the basis, the EM algorithm works as follows.
First, we initialize all the (unknown) parameters values randomly. This allows us
to have a complete specification of the mixture model, which further enables us
to use Bayes’ rule to infer which distribution is more likely to generate each word.
This prediction (i.e., E-step) essentially helps us (probabilistically) separate words
generated by the two distributions. Finally, we will collect all the probabilistically
allocated counts of words belonging to our topic word distribution and normalize
them into probabilities, which serve as an improved estimate of the parameters.
The process can then be repeated to gradually improve the parameter estimate
until the likelihood function reaches a local maximum. The EM algorithm can
guarantee reaching such a local maximum, but it cannot guarantee reaching a
global maximum when there are multiple local maxima. Due to this, we usually
repeat the algorithm multiple times with different initializations in practice, using
the run that gives the highest likelihood value to obtain the estimated parameter
values.
The EM algorithm is illustrated in Figure 17.24 where we see that a binary hidden
variable z has been introduced to indicate whether a word has been generated
from the background model (z = 1) or the topic model (z = 0). For example, the
illustration shows that the is generated from background, and thus the z value is
1.0, while text is from the topic, so its z value is 0. Note that we simply assumed
(imagined) the existence of such a binary latent variable associated with each word
p(n)(z = 0|w) = 
E-step
p(θd)p(n)(w|θd)
—
p(θd)p(n)(w|θd) + p(θB)p(w|θB)
p(n+1)(w|θd) = 
How likely w is from θd 
Initialize p(w|θd) with random values.
 
Then iteratively improve it using E-step and M-step.
 
 
Stop when likelihood doesn’t change.
The Expectation-Maximization (EM) Algorithm
the
paper
presents
a
text
mining
algorithm
for
clustering
…
Hidden variable:
z 2 [0, 1]
z
1
1
1
1
0
0
0
1
0
…
M-step
c(w, d)p(n)(z = 0|w)
—
∑w′2Vc(w′, d)p(n)(z = 0|w′)
Figure 17.24
The EM algorithm.
