52
Chapter 3
Text Data Understanding
. . . 
text 0.2
mining 0.1
association 0.01
clustering 0.02
. . .
food 0.00001
. . .
. . . 
food 0.25
nutrition 0.1
healthy 0.05
diet 0.02
. . .
text 0.00001
. . .
p(w|θ1)
p(w|θ2)
Figure 3.5
Two examples of unigram language models, representing two different topics.
Given a language model θ, in general, the probabilities of generating two dif-
ferent documents D1 and D2 would be different, i.e., p(D1 | θ) ̸= p(D2 | θ). What
kind of documents would have higher probabilities? Intuitively it would be those
documents that contain many occurrences of the high probability words accord-
ing to p(w | θ). In this sense, the high probability words of θ can indicate the topic
captured by θ.
For example, the two unigram language models illustrated in Figure 3.5 suggest
a topic about “text mining” and a topic about “health”, respectively. Intuitively, if
D is a text mining paper, we would expect p(D | θ1) > p(D | θ2), while if D′ is a blog
article discussing diet control, we would expect the opposite: p(D′ | θ1) < p(D′ | θ2).
We can also expect p(D | θ1) > p(D′ | θ1) and p(D | θ2) < p(D′ | θ2).
Now suppose we have observed a document D (e.g., a short abstract of a text
mining paper) which is assumed to be generated using a unigram language model
θ, and we would like to infer the underlying model θ (i.e., estimate the probabilities
of each word w, p(w | θ)) based on the observed D. This is a standard problem
in statistics called parameter estimation and can be solved using many different
methods.
One popular method is the maximum likelihood (ML) estimator, which seeks a
model ˆθ that would give the observed data the highest likelihood (i.e., best explain
the data):
ˆθ = arg maxθ p(D | θ).
(3.2)
It is easy to show that the ML estimate of a unigram language model gives each
word a probability equal to its relative frequency in D. That is,
p(w | ˆθ) = c(w, D)
|D|
,
(3.3)
