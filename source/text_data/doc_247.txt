11.1 Content-based Recommendation
227
for machine learning approaches, which require a large amount of training data.
In the extreme case at the beginning, we don’t even have any labeled data at all, but
the system still has to make a decision.
This issue is called the exploration-exploitation tradeoff. This means we want to
explore the document space to see if the user might be interested in the documents
that we have not yet labeled, but we don’t want to show the user too many non-
relevant documents or they will be unsatisfied with the system. So how do we do
that? We could lower the threshold a little bit and deliver some near misses to
the user to see what their response to this extra document is. This is a tradeoff
because on one hand, you want to explore, but on the other hand, you don’t
want to explore too much since you would over-deliver non-relevant information.
Exploitation means you would take advantage of the information learned about the
user. Say you know the user is interested in this particular topic, so you don’t want
to deviate that much. However, if you don’t deviate at all, then you don’t explore
at all, and you might miss the opportunity to learn another interest of the user.
Clearly, this is a dilemma and a difficult problem to solve.
Why don’t we just use the empirical utility optimization strategy to optimize
U? The problem is that this strategy is used to optimize the threshold based on
historical data. That is, you can compute the utility on the training data for each
candidate score threshold, keeping track of the highest utility observed given a
θ. This doesn’t account for the exploration that we just mentioned, and there is
also the difficulty of biased training samples. In general, we can only get an upper
bound for the true optimal threshold because the threshold might be lower than we
found; it’s possible that some of the discarded items might actually be interesting
to the user. So how do we solve this problem? We can lower the threshold to explore
a little bit. We’ll discuss one particular approach called beta-gamma threshold
learning [Zhai et al. 1998].
The basic idea of the beta-gamma threshold learning algorithm is as follows.
Given a ranked list of all the documents in the training database sorted by their
scores on the x-axis, their relevance, and a specific utility U, we can plot the utility
value at each different cutoff position θ. Each cutoff position corresponds to a score
threshold. Figure 11.5 shows this configuration and how a choice of α determines
a cutoff point between the optimal and the zero utility points, and how β and γ
help us to adjust α dynamically according to the number of judged examples in
the training database. The optimal point θopt is the point when we would achieve
the maximum utility if we had chosen this threshold. The θzero threshold is the
zero utility threshold. Between these two θ values give us a safe point to explore
the potential cutoff values. As one can see from the formula, the threshold will be
