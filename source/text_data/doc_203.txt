9.5 Practical Issues in Evaluation
183
for NDCG in order to normalize our own DCG in the range [0, 1]. Essentially, we
compare the actual DCG with the best result you can possibly get for this query.
Thisdoesn’taffecttherelativecomparisonofsystemsforjustonetopicbecausethis
ideal DCG is the same for all the systems. The difference is when we have multiple
topics—if we don’t do normalization, different topics will have different scales of
DCG. For a query like this one, we have nine highly relevant documents, but of
course that will not always be the case.
Thus, NDCG is used for measuring relevance based on much more than one
relevance level. In a more general way, this is basically a measure that can be applied
through any ranked task with a large range of judgments. Furthermore, the scale
of the judgments can be dependant on the application at hand. The main idea of
this measure is to summarize the total utility of the top k documents; you always
choose a cutoff, and then you measure the total utility. It discounts the contribution
from lowly ranked documents, and finally, it performs normalization to ensure
comparability across queries.
9.5
Practical Issues in Evaluation
In order to create a test collection, we have to create a set of queries, a set of
documents, and a set of relevance judgments. It turns out that each requirement
has its own challenges.
First, the documents and queries must be representative. They must represent
real queries and real documents that users interact with. We also have to use many
queries and many documents in order to avoid biased conclusions. In order to
evaluate a high-recall retrieval task, we must ensure there exist many relevant doc-
uments for each query. If a query has only one relevant document in the collection,
then it’s not very informative to compare different methods using such a query
because there is not much room to see a difference.
In terms of relevance judgements, the challenge is to ensure complete judge-
ments of all the documents for all the queries while simultaneously minimizing
human effort. Because we have to use human effort to label these documents, it’s
a very labor-intensive task. As a result, it’s usually impossible to actually label all of
the documents for all the queries, especially considering a data set like the Web.
It’s also challenging to correlate the evaluation measures with the perceived
utility of users. We have to consider carefully what the users care about and then
design measures to capture their preferences.
With a certain probability, we can mathematically quantify whether the eval-
uation scores of two systems are indeed different. The way we do this is with a
