17.3 Mining One Topic from Text
365
E-step:
p(n)(z = 0 | w) =
p(θd)p(n)(w | θd)
p(θd)p(n)(w | θd) + p(θB)p(w | θB)
M-step:
p(n+1)(w | θd) =
c(w, d)p(n)(z = 0 | w)
�
w′∈V c(w′, d)p(n)(z = 0 | w′)
Assume p(θd) = p(θB) = 0.5 and p(w | θB) is known.
Iteration 1
Iteration 2
Iteration 3
Word
No.
p(w | θB)
P(w | θ)
p(z = 0 | w)
P (w | θ)
P(z = 0 | w)
P (w | θ)
P(z = 0 | w)
The
4
0.5
0.25
0.33
0.20
0.29
0.18
0.26
Paper
2
0.3
0.25
0.45
0.14
0.32
0.10
0.25
Text
4
0.1
0.25
0.71
0.44
0.81
0.50
0.93
Mining
2
0.1
0.25
0.71
0.22
0.69
0.22
0.69
Log-Likelihood
−16.96
−16.13
−16.02
Likelihood increasing −→
Figure 17.25
An example of EM computation.
are shown in the second column of the table. The EM algorithm then initializes all
the parameters to be estimated. In our case, we set all the probabilities to 0.25 in
the fourth column of the table.
In the first iteration of the EM algorithm, we will apply the E-step to infer
which of the two distributions has been used to generate each word, i.e., to com-
pute p(z = 0 | w) and p(z = 1 | w). We only showed p(z = 0 | w), which is needed
in our M-step (p(z = 1 | w) = 1 − p(z = 0 | w)). Clearly, p(z = 0 | w) has different
values for different words, and this is because these words have different prob-
abilities in the background model and the initialized θd. Thus, even though the
two distributions are equally likely (by our prior) and our initial values for p(w | θd)
form a uniform distribution, the inferred p(z = 0 | w) would tend to give words
with smaller probabilities if p(w | θB) give them a higher probability. For example,
p(z = 0 | text) > p(z = 0 | the).
Once we have the probabilities of all these z values, we can perform the M-step,
where these probabilities would be used to adjust the counts of the corresponding
words. For example, the count of the is 4, but since p(z = 0 | the) = 0.33, we would
obtainadiscountedcountofthe, 4 × 0.33, whenestimatingp(the | θd)intheM-step.
Similarly, the adjusted count for text would be 4 × 0.71. After the M-step, p(text | θd)
would be much higher than p(the | θd) as shown in the table (shown in the first
