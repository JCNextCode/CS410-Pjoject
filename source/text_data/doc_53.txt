2.2 Information Theory
33
1.0
0.8
0.6
0.4
0.2
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
H(X)
P(X = 1)
Figure 2.1
Entropy as a measure of randomness of a random variable.
easier to predict than the outcome of Z. This is precisely what entropy captures.
You can calculate H(Y) and H(Z) to confirm this answer.
For our applications, it may be useful to consider the entropy of a word w in
some context. Here, high-entropy words would be harder to predict. Let W be the
random variable that denotes whether a word occurs in a document in our corpus.
Say W = 1 if the word occurs and W = 0 otherwise. How do you think H(Wthe)
compares to H(Wcomputer)? The entropy of the word the is close to zero since it
occurs everywhere. Itâ€™s not surprising to see this word in a document, thus it is easy
to predict that Wthe = 1. This case is just like the biased coin that always lands one
way. The word computer, on the other hand, is a less common word and is harder
to predict whether it occurs or not, so the entropy will be higher.
When we attempt to quantify uncertainties of conditional probabilities, we can
also define conditional entropy H(X | Y), which indicates the expected uncertainty
of X given that we observe Y, where the expectation is taken under the distribution
of all possible values of Y. Intuitively, if X is completely determined by Y, then
H(X | Y) = 0 since once we know Y, there would be no uncertainty in X, whereas if
X and Y are independent, then H(X | Y) would be the same as the original entropy
of X, i.e., H(X | Y) = H(X) since knowing Y does not help at all in resolving the
uncertainty of X.
