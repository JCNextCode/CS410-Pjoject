170
Chapter 9
Search Engine Evaluation
These Qi judgements are created by users that interact with each system. Once we
have these judgements, we can compare two or more systems. Each query is run on
each system, and we investigate the documents that each system returns.
Let’s say the query is Q1. In the figure we have RA as ranked results from system A
and RB as the ranked results from system B. Thus, RA is system A’s approximation
of relevant documents and RB is system B’s approximation. Let’s take a look at
these results—which is better? As a user, which one would you like? There are
some differences and there are some documents that are returned by both systems.
But if you look at the results you will feel that maybe system A is better in the
sense that we don’t have that many documents returned, and among the three
documents returned two of them are relevant. That’s good; system A is precise. On
the other hand, we can also say system B is better because it returned more relevant
documents; it returned three instead of two. So which one is better and how do we
quantify this? This question highly depends on a user’s task, and it depends on the
individual users as well! For some users, maybe system A is better if the user is not
interested in getting all the relevant documents so he or she doesn’t have to read
too much. On the other hand, imagine a user might need to have as many relevant
documents as possible, for example, in writing a literature survey. You might be
in the second category, and then you might find that system B is better. In either
case, we’ll have to also define measures that would quantify the information need
of a user. We will need to define multiple measures because users have different
perspectives when looking at the results.
9.2
Evaluation of Set Retrieval
In this section, we examine the basic measures for evaluation of text retrieval
systems. We discuss how to design these basic measures and how to quantitatively
compare two systems. Although the systems return a ranked list of documents, the
evaluation metrics discussed in this section deal with sets of returned documents;
that is, the order of the returned results is not taken into account. These measures
and their intuition are used to design other more sophisticated methods, but are
also quite valuable on their own.
9.2.1
Precision and Recall
Let’s return to Figure 9.1. Which set of results is better—system A’s or system
B’s? We can now discuss how to actually quantify their performance. Suppose we
have a total of ten relevant documents in the corpus for the current query, Q1. Of
course, the relevance judgements shown on the right did not include all the ten.
