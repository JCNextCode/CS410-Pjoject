14.3 Term Clustering
291
Word Pair
Mutual Information
Humpty Dumpty
22.5
Klux Klan
22.2
Ku Klux
22.2
Chah NuIth
22.2
Lao Bao
22.2
Nuu Chah
22.1
Tse Tung
22.1
avant garde
22.1
Carena Bancorp
22.0
gizzard shad
22.0
Bobby Orr
22.0
Warnok Hersey
22.0
mutatis murtandis
21.9
Taj Mahal
21.8
Figure 14.7
Sample non-compositional phrases discovered using n-gram class language model.
(From Brown et al. [1992])
positions. When the window of co-occurrences is restricted to two words (i.e.,
adjacent co-occurrences), the model can discover “sticky phrases” (see Figure 14.7
for sample results), which are non-compositional phrases whose meaning is not a
direct composition of the meanings of individual words. Such non-compositional
phrases can also be discovered using some other statistical methods (see, e.g., Zhai
1997, Lin 1999).
14.3.3.2
Neural language model (word embedding)
In Chapter 13, we discussed in length how to represent a term as a term vector based
on the words in the context where the term occurs, and compute term similarity
based on the similarity of their vector representations. Such a contextual view of
term representation can not only be used for discovering paradigmatic relations,
but also support term clustering in general since we can use any document cluster-
ing algorithm by viewing a term as a “document” represented by a vector. It can also
help word sense disambiguation since when an ambiguous word takes a different
