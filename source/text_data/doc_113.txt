6.3 Vector Space Retrieval Models
93
6.3.1
Instantiation of the Vector Space Model
In this section, we will discuss how to instantiate a vector space model so that we
can get a very specific ranking function. As mentioned previously, the vector space
model is really a framework: it doesn’t specify many things. For example, it did not
say how we should define the dimensions of the vectors. It also did not say how
we place a document vector or query vector into this space. That is, how should we
define/calculate the values of all the elements in the query and document vectors?
Finally, it did not say how we should compute similarity between the query vector
and the document vector. As you can imagine, in order to implement this model,
we have to determine specifically how we should compute and use these vectors.
In Figure 6.3, we illustrate the simplest instantiation of the vector space model.
In this instantiation, we use each word in our vocabulary to define a dimension,
thus giving |V | dimensions—this is the bag-of-words instantiation. Now let’s look
at how we place vectors in this space. Here, the simplest strategy is to use a bit vector
to represent both a query and a document, and that means each element xi and yi
would take a value of either zero or one. When it’s one, it means the corresponding
word is present in the document or query. When it’s zero, it’s absent. If the user
types in a few words for a query, then the query vector would have a few ones and
many, many zeros. The document vector in general would have more ones than the
query vector, but there will still be many zeros since the vocabulary is often very
large. Many words in the vocabulary don’t occur in a single document; many words
will only occasionally occur in a given document. Most words in the vocabulary will
be absent in any particular document.
Now that we have placed the documents and the query in the vector space, let’s
look at how we compute the similarity between them. A commonly used similarity
measure is the dot product; the dot product of two vectors is simply defined as the
sum of the products of the corresponding elements of the two vectors. In Figure 6.3
we see that it’s the product of x1 and y1 plus the product of x2 and y2, and so on.
q = (x1, …, xN)
Sim(q, d) = q.d = x1y1 + … + xNyN = ΣN
i=1 xi yi
xi, yi 2{0, 1}
1: word Wi is present
0: word Wi is absent
d = (y1, …, yN)
Figure 6.3
Computing the similarity between a query and document vector using a bit vector
representation and dot product similarity.
