17.3 Mining One Topic from Text
351
What parameters do we have in such a two-component mixture model? In Fig-
ure 17.16, we summarize the mixture of two unigram language models, list all the
parameters, and illustrate the parameter estimation problem. First, our data is just
one document d, and the model is a mixture model with two components. Second,
the parameters include two unigram language models and a distribution (mixing
weight) over the two language models. Mathematically, θd denotes the topic of doc-
ument d while θB represents the background word distribution, which we can set
to a fixed word distribution with high probabilities on common words. We denote
all the parameters collectively by �. (Can you see how many parameters exactly we
have in total?)
The figure also shows the derivation of the likelihood function. The likelihood
function is seen to be a product over all the words in the document, which is exactly
the same as in the case of a simple unigram language model. The only difference
is that inside the product, it’s now a sum instead of just one probability as in the
simple unigram language model. We have this sum due to the mixture model where
we have an uncertainty in using which model to generate a data point. Because of
this uncertainty, our likelihood function also contains a parameter to denote the
probability of choosing each particular component distribution. The second line of
the equation for the likelihood function is just another way of writing the product,
which is now a product over all the unique words in our vocabulary instead of over
all the positions in the document as in the first line of the equation.
We have two types of constraints: one is that all the word distributions must sum
to one, and the other constraint is that the probabilities of choosing each topic
must sum to one. The maximum likelihood estimation problem can now be seen
as a constrained optimization problem where we seek parameter values that can
maximize the likelihood function and satisfy all the constraints.
17.3.3
Estimation of a mixture model
In this section, we will discuss how to estimate the parameters of a mixture model.
We will start with the simplest scenario where one component (the background)
is already completely known, and the topic choice distribution has an equal prob-
ability of choosing either the background or the topic word distribution. Our goal
is to estimate the unknown topic word distribution where we hope to not see com-
mon words with high probabilities. A main assumption is that those common
words are generated using the background model, while the more discriminative
content-bearing words are generated using the (unknown) topic word distribution,
