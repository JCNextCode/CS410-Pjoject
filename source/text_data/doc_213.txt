10.1 Web Crawling
193
site in circles? Yet another issue is that we don’t want to overload one particular
server with too many crawling requests. Those may cause the site to experience
a denial of service; some sites will also block IP addresses that they believe to be
crawling them or creating too many requests. In a similar vein, a crawler should
respect the robot exclusion protocol. A file called robots.txt at the root of the site
tells crawlers which paths they are not allowed to crawl. You also need to handle
different types of files such as images, PDFs, or any other kinds of formats on the
web that contain useful information for your search application. Ideally, the crawler
should recognize duplicate pages so it doesn’t repeat itself or get stuck in a loop.
Finally, it may be useful to discover hidden URLs; these are URLs that may not be
linked from any page yet still contain content that you’d like to index.
So, what are the major crawling strategies? In general, breadth-first search is the
most common because it naturally balances server load. Parallel crawling is also
very natural because this task is very easy to parallelize. One interesting variation is
called focused crawling. Here, we’re going to crawl some pages about a particular
topic, e.g., all pages about automobiles. This is typically going to start with a query
that you use to get some results. Then, you gradually crawl more. An even more
extreme version of focused crawling is (for example) downloading and indexing all
forum posts on a particular forum. In this case, we might have a URL such as
http://www.text-data-book-forum.com/boards?id=3
which refers to the third post on the forum. By changing the id parameter, we
can iterate through all forum posts and index them quite easily. In this scenario,
it’s especially important to add a delay between requests so that the server is not
overwhelmed.
Another challenge in crawling is to find new pages that have been created since
the crawler last ran. This is very challenging if the new pages have not been linked
to any old page. If they are, then you can probably find them by following links from
existing pages in your index.
Finally, we might face the scenario of incremental crawling or repeated crawling.
Let’s say you want to be able to create a web search engine. Clearly, you first crawl
data from the web. In the future we just need to crawl the updated pages. This is
a very interesting research question: how can we determine when a page needs to
be recrawled (or even when a new page has been created)? There are two major
factors to consider here, the first of which is whether a particular page would be
updated frequently. If the page is a static page that hasn’t been changed for months,
it’s probably not necessary to re-crawl it every day since it’s unlikely that it will be
changed frequently. On the other hand, if it’s (for example) a sports score page
