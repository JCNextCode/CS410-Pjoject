358
Chapter 17
Topic Analysis
word occurs, the higher its overall probability should be. This is, in fact, a very gen-
eral phenomenon of all the maximum likelihood estimators. In our special case, if
a word occurs more frequently in the observed text data, it would also encourage
the unknown distribution θd to assign a somewhat higher probability to this word.
We can also use this example to examine the impact of p(θB), the probability of
choosing the background model. We’ve been so far assuming that each model is
equally likely, i.e., p(θB) = 0.5. But, you can again look at this likelihood function
shown in Figure 17.21 and try to picture what would happen to the likelihood
function if we increase the probability of choosing the background model.
It is not hard to notice that if p(θB) > 0.5 is set to a very large value, then all
the terms representing the probability of the would be even larger because the
background has a very high probability for the (0.9), and the coefficient in front
of 0.9, which was 0.5, would be even larger.
The consequence is that it is now less important for θd to increase the probability
mass for the even when we add more and more occurrences of the to the document.
This is because the overall probability of the is already very large (due to the large
p(θB) and large p(the | θB)), and the impact of increasing p(the | θd) is regulated
by the coefficient p(θd) which would be small if we make p(θB) very large. It would
be more beneficial for θd to ensure p(text | θd) to be high since text does not get
any help from the background model, and it must rely on θd to assign a high
probability. While high frequency words tend to get higher probabilities in the
estimated p(w | θd), the degree of increase of probability due to the increased
counts of a word observed in the document is regularized by p(θd) (or equivalently
p(θB)). The smaller p(θd) is, the less important for θd to respond to the increase
of counts of a word in the data. In general, the more likely a component is being
chosen in a mixture model, the more important it is for the component model to
assign higher probability values to these frequent words.
To summarize, we discussed the mixture model, the estimation problem of the
mixture model, and some general behaviors of the maximum likelihood estimator.
First, every component model attempts to assign high probabilities to high fre-
quent words in the data so as to collaboratively maximize the likelihood. Second,
different component models tend to bet high probabilities on different words in
order to avoid the “competition,” or waste of probability. This would allow them
to collaborate more efficiently to maximize the likelihood. Third, the probabil-
ity of choosing each component regulates the collaboration and the competition
between component models. It would allow some component models to respond
more to the change of frequency of a word in the data. We also discussed the special
case of fixing one component to a background word distribution, which can be es-
timated based on a large collection of English documents using the simplest single
