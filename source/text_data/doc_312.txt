292
Chapter 14
Text Clustering
sense, it tends to “attract” different words in its surrounding text, thus would have
a different context representation.
This technique is not limited to unigram words, and we can think of other
representations for the vector such as part-of-speech tags or even elements like
sentiment. Adding these additional features means expanding the word vector
from |V | to whatever size we require. Additionally, aside from finding semantically-
related terms, using this richer word representation has the ability to improve
downstream tasks such as grammatical parsing or statistical machine translation.
However, the heuristic way to obtain vector representation discussed in Chap-
ter 13 has the disadvantage that we need to make many ad hoc choices, especially
in how to obtain the term weights. Another deficiency is that the vector spans the
entire space of words in the vocabulary, increasing the complexity of any further
processing applied to the vectors.
As an alternative, we can use a neural language model [Mikolov et al. 2010] to sys-
tematically learn a vector representation for each word by optimizing a meaningful
objective function. Such an approach is also called word embedding, which refers to
the mapping of a word into a vector representation in a low-dimensional space. The
general idea of these methods is to assume that each word corresponds to a vector
in an unknown (latent) low-dimensional space and define a language model solely
based on the vector representations of the involved words so that the parameters for
such a language model would be the vector representations of words. As a result, by
fitting the model to a specific data set, we can learn the vector representations for
all the words. These language models are called neural language models because
they can be represented as a neural network. For example, to model an n-gram lan-
guage model p(wn | wn−1, . . . , w1), the neural network would have wn−1, . . . , w1 as
input and wn as the output. In some neural language models, the hidden layer in
the neural network connected to a word can be interpreted as a vector represen-
tation of the word with the elements being the weights on the edges connected to
the word.
For example, in the skip-gram neural language model [Mikolov et al. 2013], the
objective function is to use each word to predict all other words in its context as
defined by a window around the word, and the probability of predicting word w1
given word w2 is given by
p(w1 | w2) =
exp( ⃗v1. ⃗v2)
�
wi∈V exp( ⃗vi. ⃗v2)
where vi is the corresponding vector representation of word wi. In words, such a
model says that the probability p(w1 | w2) is proportional to the dot product of the
