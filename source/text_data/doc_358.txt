338
Chapter 17
Topic Analysis
�
w∈V
p(w | θi) = 1.
(17.1)
Naturally, we still have the same constraint on the topic coverage, i.e.,
k
�
j=1
πij = 1, ∀i.
(17.2)
As a computation problem, our input is text data, a collection of documents C,
and we assume that we know the number of topics, k, or hypothesize that there
are k topics in the text data. As part of our input, we also know the vocabulary V ,
which determines what units would be treated as the basic units (i.e., words) for
analysis. In most cases, we will use words as the basis for analysis simply because
they are the most natural units, but it is easy to generalize such an approach to
use phrases or any other units that we can identify in text, as the basic units and
treat them as if they were words. Our output consists of two families of probability
distributions. The first is a set of topics represented by a set of θi’s, each of which is
a word distribution. The second is a topic coverage distribution for each document
di, {πi1, . . . , πik}.
The question now is how to generate such output from our input. There are
potentially many different ways to do this, but here we introduce a general way of
solving this problem called a generative model. This is, in fact, a very general idea
and a principled way of using statistical modeling to solve text mining problems.
The basic idea of this approach is to first design a generative model for our data,
i.e., a probabilistic model to model how the data are generated, or a model that can
allow us to compute the probability of how likely we will observe the data we have.
The actual data aren’t necessarily (indeed often unlikely) generated this way, but
by assuming the data to be generated in a particular way according to a particular
model, we can have a formal way to characterize our data which further facilitates
topic discovery.
In general, our model will have some parameters (which can be denoted by �);
they control the behavior of the model by controlling what kind of data would have
high (or low) probabilities. If you set these parameters to different values, the model
would behave differently; that is, it would tend to give different data points high (or
low) probabilities.
We design the model in such a way that its parameters would encode the knowl-
edge we would like to discover. Then, we attempt to estimate these parameters
based on the data (or infer the values of parameters based on the observed data) so
as to generate the desired output in the form of parameter values, which we have
