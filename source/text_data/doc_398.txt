378
Chapter 17
Topic Analysis
are covered in the document. Thus, we can define a prior on the topic coverage to
ensure that a document can only be generated using topics corresponding to the
tags assigned to it. This essentially gives us a constraint on what topics can be used
to generate words in a document, which can be useful for learning co-occuring
words in the context of a topic when the data are sparse and pure co-occurrence
statistics are insufficient to induce a meaningful topic.
All such prior knowledge can be incorporated into PLSA by using Maximum A
Posteriori Estimation (MAP) instead of Maximum Likelihood estimation. Specifi-
cally, we denote all the parameters by � and introduce a prior distribution p(�)
over all the possible values of � to encode our preferences. Such a prior distribu-
tion would technically include a distribution over all possible word distributions
(for topic characterization) and all possible coverage distributions of topics in a
document (for topic coverage), and can be defined based on whatever knowledge
or preferences we would like to inject into the model. With such a prior, we can
then estimate parameters by using MAP as follows:
�∗ = arg max� p(�)p(Data | �),
(17.6)
where p(Data | �) is the likelihood function, which would be the sole term to
maximize in the case of ML estimation. Adding the prior p(�) would encourage
the model to seek a compromise of the ML estimate (which maximizes p(Data | �))
and the mode of the prior (which maximizes p(�)).
There are potentially many different ways to define p(�). However, it is partic-
ularly convenient to use a conjugate prior distribution, in which the prior density
function p(�) is of the same form as the likelihood function p(Data | �) as a func-
tion of the parameter �. Due to the same form of the two functions, we can generally
merge the two to derive a single function (again, of the same form). In other words,
our posterior distribution is written as a function of the parameter, so the maxi-
mization of the posterior probability would be similar to the maximization of the
likelihood function. Since the posterior distribution is of the same form as the like-
lihood function of the original data, we can interpret the posterior distribution as
the likelihood function for an imagined pseudo data set that is formed by augment-
ing the original data with additional “pseudo data” such that the influence of the
prior is entirely captured by the addition of such pseudo data to the original data.
When using such a conjugate prior, the computation of MAP can be done by
using a slightly modified version of the EM algorithm that we introduced earlier
for PLSA where appropriate counts of pseudo data are added to incorporate the
prior. As a specific example, if we define a conjugate prior on the word distributions
