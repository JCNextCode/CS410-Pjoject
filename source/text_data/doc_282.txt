262
Chapter 13
Word Association Mining
p(Xmeat = 1)
p(Xmeat = 0)
Know nothing about the segment
H(Xmeat) = –p(Xmeat = 0) log2 p(Xmeat = 0) – p(Xmeat = 1) log2 p(Xmeat = 1)
H(Xmeat|Xeats = 1) = –p(Xmeat = 0|Xeats = 1) log2 p(Xmeat = 0|Xeats = 1)
H(Xmeat|Xeats = 0) can be deﬁned similarly
–p(Xmeat = 1|Xeats = 1) log2 p(Xmeat = 1|Xeats = 1)
p(Xmeat = 1|Xeats = 1)
p(Xmeat = 0|Xeats = 1)
Know “eats” is present (Xeats = 1)
Figure 13.9
Illustration of conditional entropy.
Now, we’ll address a different scenario where we assume that we know some-
thing about the random variable. That is, suppose we know that eats occurred in
the segment. How would that help us predict the presence or absence of a word
like meat? If we frame this question using entropy, that would mean we are inter-
ested in knowing whether knowing the presence of eats could reduce uncertainty
about meat. In other words, can we reduce the entropy of the random variable cor-
responding to the presence or absence of meat? What if we know of the absence
of eats? Would that also help us predict the presence or absence of meat? These
questions can be addressed by using conditional entropy.
To explain this concept, let’s first look at the scenario we had before, when we
know nothing about the segment. We have probabilities indicating whether a word
occurs or doesn’t occur in the segment. We have an entropy function that looks like
the one in Figure 13.9.
Suppose we know eats is present, which means we know the value of Xeats. That
fact changes all these probabilities to conditional probabilities. We look at the pres-
ence or absence of meat, given that we know eats occurred in the context. That is, we
have p(Xmeat | Xeats = 1). If we replace these probabilities with their corresponding
conditional probabilities in the entropy function, we’ll get the conditional entropy
(conditioned on the presence of eats). This is essentially the same entropy function
as before, except that all the probabilities now have a condition. This then tells us
the entropy of meat after we have known eats occurs in the segment. Of course, we
can also define this conditional entropy for the scenario where we don’t see eats.
Now, putting these different scenarios together, we have the complete definition
of conditional entropy:
